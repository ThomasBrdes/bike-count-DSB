{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa46e181",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b319720",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To get some insights\n",
    "# https://github.com/ceptln/paris-bike-traffic-prediction/tree/main\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from submissions.external_data.estimator import _encode_dates, _merge_external_data\n",
    "import utils.get_data as get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e4db5-6407-438a-b58c-59067dcdc2c9",
   "metadata": {},
   "source": [
    "### Function submission kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e0b08f4-fdc6-45d6-b968-07921e101ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_kaggle(model, X_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(y_pred)\n",
    "    results = pd.DataFrame(\n",
    "        dict(\n",
    "            Id=np.arange(y_pred.shape[0]),\n",
    "            log_bike_count=y_pred,\n",
    "        )\n",
    "    )\n",
    "    results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba175e3",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47b8f384-d4b5-42ee-b2dd-c7e13e88ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_external_data(X):\n",
    "    \n",
    "    X = X.copy()\n",
    "    \n",
    "    # merge original data + external data\n",
    "    merged_X_train_external_DATA = get_data._merge_external_data_weather(X)\n",
    "    \n",
    "    # merge original data + external data + holidays\n",
    "    merged_X_train_external_HOLIDAYS = get_data._merge_holidays_week_end(merged_X_train_external_DATA)\n",
    "    \n",
    "    # merge original data + external data + holidays + data COVID\n",
    "    merged_X_train_external_HOLIDAYS_COVID = get_data._merge_Curfews_lockdowns_COVID(merged_X_train_external_HOLIDAYS)\n",
    "    merged_X_train_external_HOLIDAYS_COVID = get_data._merge_indicators_COVID(merged_X_train_external_HOLIDAYS_COVID)\n",
    "    \n",
    "    # merge original data + external data + holidays + data COVID + data accidents\n",
    "    merged_X_train_external_HOLIDAYS_COVID_ACCIDENTS = get_data._merge_road_accidents(merged_X_train_external_HOLIDAYS_COVID)\n",
    "    merged_X_train_external_HOLIDAYS_COVID_ACCIDENTS \n",
    "    return merged_X_train_external_HOLIDAYS_COVID_ACCIDENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50fd8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "X_train, y_train = get_data.get_train_data()\n",
    "X_test, y_test = get_data.get_test_data()\n",
    "X_final_test = get_data.get_final_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95a61e5b-411e-4eb5-9870-a734d4088940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>ff</th>\n",
       "      <th>...</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_lockdown</th>\n",
       "      <th>is_curfew</th>\n",
       "      <th>hosp</th>\n",
       "      <th>rea</th>\n",
       "      <th>incid_rea</th>\n",
       "      <th>rad</th>\n",
       "      <th>Max_Grav_accidents</th>\n",
       "      <th>Count_accidents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100049407-353255860</td>\n",
       "      <td>152 boulevard du Montparnasse E-O</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>100049407-353255859</td>\n",
       "      <td>152 boulevard du Montparnasse O-E</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>100036719-104036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville NO-SE</td>\n",
       "      <td>100036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>Y2H19027732</td>\n",
       "      <td>48.853720</td>\n",
       "      <td>2.357020</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>100036719-103036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville SE-NO</td>\n",
       "      <td>100036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>Y2H19027732</td>\n",
       "      <td>48.853720</td>\n",
       "      <td>2.357020</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>100063175-353277233</td>\n",
       "      <td>20 Avenue de Clichy NO-SE</td>\n",
       "      <td>100063175</td>\n",
       "      <td>20 Avenue de Clichy</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>Y2H20073268</td>\n",
       "      <td>48.885290</td>\n",
       "      <td>2.326660</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             counter_id                       counter_name    site_id  \\\n",
       "0   100049407-353255860  152 boulevard du Montparnasse E-O  100049407   \n",
       "30  100049407-353255859  152 boulevard du Montparnasse O-E  100049407   \n",
       "31  100036719-104036719  18 quai de l'Hôtel de Ville NO-SE  100036719   \n",
       "32  100036719-103036719  18 quai de l'Hôtel de Ville SE-NO  100036719   \n",
       "33  100063175-353277233          20 Avenue de Clichy NO-SE  100063175   \n",
       "\n",
       "                        site_name                date  \\\n",
       "0   152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "30  152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "31    18 quai de l'Hôtel de Ville 2020-09-01 01:00:00   \n",
       "32    18 quai de l'Hôtel de Ville 2020-09-01 01:00:00   \n",
       "33            20 Avenue de Clichy 2020-09-01 01:00:00   \n",
       "\n",
       "   counter_installation_date counter_technical_id   latitude  longitude   ff  \\\n",
       "0                 2018-12-07          Y2H19070373  48.840801   2.333233  1.6   \n",
       "30                2018-12-07          Y2H19070373  48.840801   2.333233  1.6   \n",
       "31                2017-07-12          Y2H19027732  48.853720   2.357020  1.6   \n",
       "32                2017-07-12          Y2H19027732  48.853720   2.357020  1.6   \n",
       "33                2020-07-22          Y2H20073268  48.885290   2.326660  1.6   \n",
       "\n",
       "    ...  is_holiday  is_weekend  is_lockdown  is_curfew  hosp  rea  incid_rea  \\\n",
       "0   ...       False           0        False      False   293   42        3.0   \n",
       "30  ...       False           0        False      False   293   42        3.0   \n",
       "31  ...       False           0        False      False   293   42        3.0   \n",
       "32  ...       False           0        False      False   293   42        3.0   \n",
       "33  ...       False           0        False      False   293   42        3.0   \n",
       "\n",
       "     rad  Max_Grav_accidents  Count_accidents  \n",
       "0   6641                 0.0              0.0  \n",
       "30  6641                 0.0              0.0  \n",
       "31  6641                 0.0              0.0  \n",
       "32  6641                 0.0              0.0  \n",
       "33  6641                 0.0              0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_plus = add_external_data(X_train)\n",
    "X_train_plus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12de1990-5753-4250-8714-771742a3d921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>ff</th>\n",
       "      <th>...</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_lockdown</th>\n",
       "      <th>is_curfew</th>\n",
       "      <th>hosp</th>\n",
       "      <th>rea</th>\n",
       "      <th>incid_rea</th>\n",
       "      <th>rad</th>\n",
       "      <th>Max_Grav_accidents</th>\n",
       "      <th>Count_accidents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100049407-353255860</td>\n",
       "      <td>152 boulevard du Montparnasse E-O</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>100049407-353255859</td>\n",
       "      <td>152 boulevard du Montparnasse O-E</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>100036719-104036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville NO-SE</td>\n",
       "      <td>100036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>Y2H19027732</td>\n",
       "      <td>48.853720</td>\n",
       "      <td>2.357020</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>100036719-103036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville SE-NO</td>\n",
       "      <td>100036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>Y2H19027732</td>\n",
       "      <td>48.853720</td>\n",
       "      <td>2.357020</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>100063175-353277233</td>\n",
       "      <td>20 Avenue de Clichy NO-SE</td>\n",
       "      <td>100063175</td>\n",
       "      <td>20 Avenue de Clichy</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>Y2H20073268</td>\n",
       "      <td>48.885290</td>\n",
       "      <td>2.326660</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             counter_id                       counter_name    site_id  \\\n",
       "0   100049407-353255860  152 boulevard du Montparnasse E-O  100049407   \n",
       "30  100049407-353255859  152 boulevard du Montparnasse O-E  100049407   \n",
       "31  100036719-104036719  18 quai de l'Hôtel de Ville NO-SE  100036719   \n",
       "32  100036719-103036719  18 quai de l'Hôtel de Ville SE-NO  100036719   \n",
       "33  100063175-353277233          20 Avenue de Clichy NO-SE  100063175   \n",
       "\n",
       "                        site_name                date  \\\n",
       "0   152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "30  152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "31    18 quai de l'Hôtel de Ville 2020-09-01 01:00:00   \n",
       "32    18 quai de l'Hôtel de Ville 2020-09-01 01:00:00   \n",
       "33            20 Avenue de Clichy 2020-09-01 01:00:00   \n",
       "\n",
       "   counter_installation_date counter_technical_id   latitude  longitude   ff  \\\n",
       "0                 2018-12-07          Y2H19070373  48.840801   2.333233  1.6   \n",
       "30                2018-12-07          Y2H19070373  48.840801   2.333233  1.6   \n",
       "31                2017-07-12          Y2H19027732  48.853720   2.357020  1.6   \n",
       "32                2017-07-12          Y2H19027732  48.853720   2.357020  1.6   \n",
       "33                2020-07-22          Y2H20073268  48.885290   2.326660  1.6   \n",
       "\n",
       "    ...  is_holiday  is_weekend  is_lockdown  is_curfew  hosp  rea  incid_rea  \\\n",
       "0   ...       False           0        False      False   293   42        3.0   \n",
       "30  ...       False           0        False      False   293   42        3.0   \n",
       "31  ...       False           0        False      False   293   42        3.0   \n",
       "32  ...       False           0        False      False   293   42        3.0   \n",
       "33  ...       False           0        False      False   293   42        3.0   \n",
       "\n",
       "     rad  Max_Grav_accidents  Count_accidents  \n",
       "0   6641                 0.0              0.0  \n",
       "30  6641                 0.0              0.0  \n",
       "31  6641                 0.0              0.0  \n",
       "32  6641                 0.0              0.0  \n",
       "33  6641                 0.0              0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_plus = add_external_data(X_test)\n",
    "X_train_plus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a58b9be-ea52-4498-82eb-402560a7ecdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_lockdown</th>\n",
       "      <th>is_curfew</th>\n",
       "      <th>hosp</th>\n",
       "      <th>rea</th>\n",
       "      <th>incid_rea</th>\n",
       "      <th>rad</th>\n",
       "      <th>Max_Grav_accidents</th>\n",
       "      <th>Count_accidents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100049407-353255860</td>\n",
       "      <td>152 boulevard du Montparnasse E-O</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2021-09-10 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>365</td>\n",
       "      <td>126</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>100049407-353255859</td>\n",
       "      <td>152 boulevard du Montparnasse O-E</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2021-09-10 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>365</td>\n",
       "      <td>126</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>100036719-104036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville NO-SE</td>\n",
       "      <td>100036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville</td>\n",
       "      <td>2021-09-10 01:00:00</td>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>48.85372,2.35702</td>\n",
       "      <td>Y2H19027732</td>\n",
       "      <td>48.853720</td>\n",
       "      <td>2.357020</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>365</td>\n",
       "      <td>126</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>100036719-103036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville SE-NO</td>\n",
       "      <td>100036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville</td>\n",
       "      <td>2021-09-10 01:00:00</td>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>48.85372,2.35702</td>\n",
       "      <td>Y2H19027732</td>\n",
       "      <td>48.853720</td>\n",
       "      <td>2.357020</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>365</td>\n",
       "      <td>126</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>100063175-353277233</td>\n",
       "      <td>20 Avenue de Clichy NO-SE</td>\n",
       "      <td>100063175</td>\n",
       "      <td>20 Avenue de Clichy</td>\n",
       "      <td>2021-09-10 01:00:00</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>48.88529,2.32666</td>\n",
       "      <td>Y2H20073268</td>\n",
       "      <td>48.885290</td>\n",
       "      <td>2.326660</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>365</td>\n",
       "      <td>126</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             counter_id                       counter_name    site_id  \\\n",
       "0   100049407-353255860  152 boulevard du Montparnasse E-O  100049407   \n",
       "43  100049407-353255859  152 boulevard du Montparnasse O-E  100049407   \n",
       "31  100036719-104036719  18 quai de l'Hôtel de Ville NO-SE  100036719   \n",
       "32  100036719-103036719  18 quai de l'Hôtel de Ville SE-NO  100036719   \n",
       "33  100063175-353277233          20 Avenue de Clichy NO-SE  100063175   \n",
       "\n",
       "                        site_name                date  \\\n",
       "0   152 boulevard du Montparnasse 2021-09-10 01:00:00   \n",
       "43  152 boulevard du Montparnasse 2021-09-10 01:00:00   \n",
       "31    18 quai de l'Hôtel de Ville 2021-09-10 01:00:00   \n",
       "32    18 quai de l'Hôtel de Ville 2021-09-10 01:00:00   \n",
       "33            20 Avenue de Clichy 2021-09-10 01:00:00   \n",
       "\n",
       "   counter_installation_date         coordinates counter_technical_id  \\\n",
       "0                 2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "43                2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "31                2017-07-12    48.85372,2.35702          Y2H19027732   \n",
       "32                2017-07-12    48.85372,2.35702          Y2H19027732   \n",
       "33                2020-07-22    48.88529,2.32666          Y2H20073268   \n",
       "\n",
       "     latitude  longitude  ...  is_holiday  is_weekend  is_lockdown  is_curfew  \\\n",
       "0   48.840801   2.333233  ...       False           0        False      False   \n",
       "43  48.840801   2.333233  ...       False           0        False      False   \n",
       "31  48.853720   2.357020  ...       False           0        False      False   \n",
       "32  48.853720   2.357020  ...       False           0        False      False   \n",
       "33  48.885290   2.326660  ...       False           0        False      False   \n",
       "\n",
       "    hosp  rea  incid_rea    rad  Max_Grav_accidents  Count_accidents  \n",
       "0    365  126        5.0  21675                 0.0              0.0  \n",
       "43   365  126        5.0  21675                 0.0              0.0  \n",
       "31   365  126        5.0  21675                 0.0              0.0  \n",
       "32   365  126        5.0  21675                 0.0              0.0  \n",
       "33   365  126        5.0  21675                 0.0              0.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_test_plus = add_external_data(X_final_test)\n",
    "X_final_test_plus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ce50f3",
   "metadata": {},
   "source": [
    "##  Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "758a0e19-e3fa-480a-bb87-747d33ce4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_dates(X):\n",
    "    \n",
    "    '''\n",
    "    Splits the 'date' columns of the input DataFrame into several columns (year, month, day, weekday, hour)\n",
    "    \n",
    "    Parameters:\n",
    "        X (pd.DataFrame): the dataframe to modify\n",
    "    \n",
    "    Returns:\n",
    "        X (pd.DataFrame): the modified dataframe\n",
    "    '''\n",
    "    \n",
    "    # Duplicate X to work on it\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Create new columns with date parts from X.date\n",
    "    X.loc[:, \"year\"] = X[\"date\"].dt.year\n",
    "    X.loc[:, \"month\"] = X[\"date\"].dt.month\n",
    "    X.loc[:, \"day\"] = X[\"date\"].dt.day\n",
    "    X.loc[:, \"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X.loc[:, \"hour\"] = X[\"date\"].dt.hour\n",
    "\n",
    "    # Adding cosinus and sinus features from date variables to enhance the date periodicity\n",
    "    # X['cos_hour'] = np.cos(X['hour']*(2.*np.pi/24))\n",
    "    # X['sin_hour'] = np.sin(X['hour']*(2.*np.pi/24))\n",
    "    # X['cos_day'] = np.cos(X['day']*(2.*np.pi/30))\n",
    "    # X['sin_day'] = np.sin(X['day']*(2.*np.pi/30))\n",
    "    # X['cos_month'] = np.cos(X['month']*(2.*np.pi/12))\n",
    "    # X['sin_month'] = np.sin(X['month']*(2.*np.pi/12))\n",
    "    # X['cos_weekday'] = np.cos(X['weekday']*(2.*np.pi/7))\n",
    "    # X['sin_weekday'] = np.sin(X['weekday']*(2.*np.pi/7))\n",
    "    \n",
    "    # Clean the new dataframe and return it\n",
    "    X.drop(columns=[\"date\"], inplace=True)\n",
    "    #X.drop(columns=[\"year\", 'month', 'day', 'weekday', 'hour'], inplace=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f3256",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1acda-3e7e-4112-89df-4b66b0c1042f",
   "metadata": {},
   "source": [
    "## Training without pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dfe8ea3-9a9c-4d33-bb3f-b24192b9d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RMSE_local_wt_pipe(model, X_train, y_train, X_test, y_test):\n",
    "    n_folds = 5\n",
    "\n",
    "    # Perform cross-validation and compute the scores for the training set\n",
    "    cv_scores_train = cross_val_score(model, X_train, y_train, cv=n_folds, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # Perform cross-validation and compute the scores for the testing set\n",
    "    cv_scores_test = cross_val_score(model, X_test, y_test, cv=n_folds, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # Convert the scores to root mean squared error\n",
    "    rmse_scores_train = np.sqrt(-cv_scores_train)\n",
    "    rmse_scores_test = np.sqrt(-cv_scores_test)\n",
    "\n",
    "    print(f\"Train set, RMSE={np.mean(rmse_scores_train):.2f}\")\n",
    "    print(f\"Test set, RMSE={np.mean(rmse_scores_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd46e64-3e0d-4e86-a0e6-8009638ba541",
   "metadata": {},
   "source": [
    "### Select features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e68b1a9e-17ce-4350-a601-423f9281f4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>u</th>\n",
       "      <th>rr3</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_lockdown</th>\n",
       "      <th>is_curfew</th>\n",
       "      <th>hosp</th>\n",
       "      <th>rea</th>\n",
       "      <th>rad</th>\n",
       "      <th>...</th>\n",
       "      <th>counter_name_Totem 64 Rue de Rivoli E-O</th>\n",
       "      <th>counter_name_Totem 64 Rue de Rivoli O-E</th>\n",
       "      <th>counter_name_Totem 73 boulevard de Sébastopol N-S</th>\n",
       "      <th>counter_name_Totem 73 boulevard de Sébastopol S-N</th>\n",
       "      <th>counter_name_Totem 85 quai d'Austerlitz NO-SE</th>\n",
       "      <th>counter_name_Totem 85 quai d'Austerlitz SE-NO</th>\n",
       "      <th>counter_name_Totem Cours la Reine E-O</th>\n",
       "      <th>counter_name_Totem Cours la Reine O-E</th>\n",
       "      <th>counter_name_Voie Georges Pompidou NE-SO</th>\n",
       "      <th>counter_name_Voie Georges Pompidou SO-NE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>285.75</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>6641</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>285.75</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>6641</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>285.75</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>6641</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>285.75</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>6641</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>285.75</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>293</td>\n",
       "      <td>42</td>\n",
       "      <td>6641</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455129</th>\n",
       "      <td>291.45</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>285</td>\n",
       "      <td>80</td>\n",
       "      <td>21150</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455130</th>\n",
       "      <td>291.45</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>285</td>\n",
       "      <td>80</td>\n",
       "      <td>21150</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455119</th>\n",
       "      <td>291.45</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>285</td>\n",
       "      <td>80</td>\n",
       "      <td>21150</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455136</th>\n",
       "      <td>291.45</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>285</td>\n",
       "      <td>80</td>\n",
       "      <td>21150</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455162</th>\n",
       "      <td>291.45</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>285</td>\n",
       "      <td>80</td>\n",
       "      <td>21150</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455163 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             t   u  rr3  is_holiday  is_weekend  is_lockdown  is_curfew  hosp  \\\n",
       "0       285.75  81  0.0       False           0        False      False   293   \n",
       "30      285.75  81  0.0       False           0        False      False   293   \n",
       "31      285.75  81  0.0       False           0        False      False   293   \n",
       "32      285.75  81  0.0       False           0        False      False   293   \n",
       "33      285.75  81  0.0       False           0        False      False   293   \n",
       "...        ...  ..  ...         ...         ...          ...        ...   ...   \n",
       "455129  291.45  72  0.0       False           0        False      False   285   \n",
       "455130  291.45  72  0.0       False           0        False      False   285   \n",
       "455119  291.45  72  0.0       False           0        False      False   285   \n",
       "455136  291.45  72  0.0       False           0        False      False   285   \n",
       "455162  291.45  72  0.0       False           0        False      False   285   \n",
       "\n",
       "        rea    rad  ...  counter_name_Totem 64 Rue de Rivoli E-O  \\\n",
       "0        42   6641  ...                                    False   \n",
       "30       42   6641  ...                                    False   \n",
       "31       42   6641  ...                                    False   \n",
       "32       42   6641  ...                                    False   \n",
       "33       42   6641  ...                                    False   \n",
       "...     ...    ...  ...                                      ...   \n",
       "455129   80  21150  ...                                    False   \n",
       "455130   80  21150  ...                                    False   \n",
       "455119   80  21150  ...                                    False   \n",
       "455136   80  21150  ...                                    False   \n",
       "455162   80  21150  ...                                    False   \n",
       "\n",
       "        counter_name_Totem 64 Rue de Rivoli O-E  \\\n",
       "0                                         False   \n",
       "30                                        False   \n",
       "31                                        False   \n",
       "32                                        False   \n",
       "33                                        False   \n",
       "...                                         ...   \n",
       "455129                                    False   \n",
       "455130                                    False   \n",
       "455119                                    False   \n",
       "455136                                    False   \n",
       "455162                                    False   \n",
       "\n",
       "        counter_name_Totem 73 boulevard de Sébastopol N-S  \\\n",
       "0                                                   False   \n",
       "30                                                  False   \n",
       "31                                                  False   \n",
       "32                                                  False   \n",
       "33                                                  False   \n",
       "...                                                   ...   \n",
       "455129                                              False   \n",
       "455130                                              False   \n",
       "455119                                              False   \n",
       "455136                                              False   \n",
       "455162                                              False   \n",
       "\n",
       "        counter_name_Totem 73 boulevard de Sébastopol S-N  \\\n",
       "0                                                   False   \n",
       "30                                                  False   \n",
       "31                                                  False   \n",
       "32                                                  False   \n",
       "33                                                  False   \n",
       "...                                                   ...   \n",
       "455129                                              False   \n",
       "455130                                              False   \n",
       "455119                                              False   \n",
       "455136                                              False   \n",
       "455162                                              False   \n",
       "\n",
       "        counter_name_Totem 85 quai d'Austerlitz NO-SE  \\\n",
       "0                                               False   \n",
       "30                                              False   \n",
       "31                                              False   \n",
       "32                                              False   \n",
       "33                                              False   \n",
       "...                                               ...   \n",
       "455129                                          False   \n",
       "455130                                          False   \n",
       "455119                                          False   \n",
       "455136                                          False   \n",
       "455162                                          False   \n",
       "\n",
       "        counter_name_Totem 85 quai d'Austerlitz SE-NO  \\\n",
       "0                                               False   \n",
       "30                                              False   \n",
       "31                                              False   \n",
       "32                                              False   \n",
       "33                                              False   \n",
       "...                                               ...   \n",
       "455129                                           True   \n",
       "455130                                          False   \n",
       "455119                                          False   \n",
       "455136                                          False   \n",
       "455162                                          False   \n",
       "\n",
       "        counter_name_Totem Cours la Reine E-O  \\\n",
       "0                                       False   \n",
       "30                                      False   \n",
       "31                                      False   \n",
       "32                                      False   \n",
       "33                                      False   \n",
       "...                                       ...   \n",
       "455129                                  False   \n",
       "455130                                   True   \n",
       "455119                                  False   \n",
       "455136                                  False   \n",
       "455162                                  False   \n",
       "\n",
       "        counter_name_Totem Cours la Reine O-E  \\\n",
       "0                                       False   \n",
       "30                                      False   \n",
       "31                                      False   \n",
       "32                                      False   \n",
       "33                                      False   \n",
       "...                                       ...   \n",
       "455129                                  False   \n",
       "455130                                  False   \n",
       "455119                                   True   \n",
       "455136                                  False   \n",
       "455162                                  False   \n",
       "\n",
       "        counter_name_Voie Georges Pompidou NE-SO  \\\n",
       "0                                          False   \n",
       "30                                         False   \n",
       "31                                         False   \n",
       "32                                         False   \n",
       "33                                         False   \n",
       "...                                          ...   \n",
       "455129                                     False   \n",
       "455130                                     False   \n",
       "455119                                     False   \n",
       "455136                                      True   \n",
       "455162                                     False   \n",
       "\n",
       "        counter_name_Voie Georges Pompidou SO-NE  \n",
       "0                                          False  \n",
       "30                                         False  \n",
       "31                                         False  \n",
       "32                                         False  \n",
       "33                                         False  \n",
       "...                                          ...  \n",
       "455129                                     False  \n",
       "455130                                     False  \n",
       "455119                                     False  \n",
       "455136                                     False  \n",
       "455162                                      True  \n",
       "\n",
       "[455163 rows x 101 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_plus_chosen_FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1eea8137-4c10-4cb0-98cf-5dd5274aa6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_features = ['date', \"site_name\", \"t\"]\n",
    "\n",
    "not_fixed_features = [\"counter_id\", \"counter_technical_id\", \"counter_installation_date\", \"counter_name\",\n",
    "                      'site_id', \n",
    "                      'site_name'\n",
    "                       'counter_installation_date', \n",
    "                      'counter_technical_id', \n",
    "                      'latitude',\n",
    "                       'longitude', \n",
    "                      'ff', # the wind speed\n",
    "                      'u', # the humidity\n",
    "                      'ssfrai', # the fresh snowfall amount\n",
    "                      'n', # the amount of cloud cover\n",
    "                      'vv', # the visibility\n",
    "                      'rr3', # the precipitation amount over 3 hours\n",
    "                      't', # the temperature\n",
    "                      'is_holiday', # is holidays\n",
    "                       'is_weekend', # is week end\n",
    "                      'is_lockdown', # Lockdown for COVID\n",
    "                      'is_curfew', # Curfew for COVID\n",
    "                      'hosp', # Number of patients currently hospitalised for COVID-19\n",
    "                      'rea', # Number of patients currently in intensive care.\n",
    "                      'incid_rea', # Number of new patients admitted to intensive care in the last 24 hours.\n",
    "                       'rad', # Cumulative number of patients hospitalised for COVID-19 who have returned home due to an improvement in their state of health\n",
    "                      'Max_Grav_accidents', # The maximum severity of all cyclists accidents at a given hour\n",
    "                      'Count_accidents' # the number of accidents at a given hour in Paris\n",
    "                     ]\n",
    "\n",
    "\n",
    "chosen_not_fixed_features = ['u', 'rr3', 'is_holiday', \"counter_name\",\n",
    "       'is_weekend', 'is_lockdown', 'is_curfew', 'hosp', 'rea',\n",
    "       'rad']\n",
    "\n",
    "# VARIABLES REMOVED BY rfecv\n",
    "# {'Count_accidents',\n",
    "#  'Max_Grav_accidents',\n",
    "#  'cos_day',\n",
    "#  'ff',\n",
    "#  'incid_rea',\n",
    "#  'is_curfew',\n",
    "#  'is_lockdown',\n",
    "#  'n',\n",
    "#  'sin_day',\n",
    "#  'sin_weekday',\n",
    "#  'site_name_39 quai François Mauriac',\n",
    "#  'site_name_6 rue Julia Bartet',\n",
    "#  'site_name_90 Rue De Sèvres',\n",
    "#  'ssfrai',\n",
    "#  'vv'}\n",
    "\n",
    "\n",
    "chosen_variables = fixed_features + chosen_not_fixed_features\n",
    "\n",
    "X_train_plus_chosen = X_train_plus[chosen_variables]\n",
    "X_test_plus_chosen = X_test_plus[chosen_variables]\n",
    "X_final_test_plus_chosen = X_final_test_plus[chosen_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4cc7058-a45f-4136-9eb5-f5d645ac2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying continuous variables (float type or int with wide range)\n",
    "continuous_columns  = X_train_plus_chosen_FI.select_dtypes(include=['float64', 'float32', 'int64', 'int32']).columns.tolist()\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_plus_chosen_FI = _encode_dates(X_train_plus_chosen)\n",
    "X_train_plus_chosen_FI = pd.get_dummies(X_train_plus_chosen_FI, columns=['site_name'])\n",
    "X_train_plus_chosen_FI = pd.get_dummies(X_train_plus_chosen_FI, columns=['counter_name'])\n",
    "X_train_plus_chosen_FI[continuous_columns] = scaler.fit_transform(X_train_plus_chosen_FI[continuous_columns])\n",
    "\n",
    "X_test_plus_chosen_FI = _encode_dates(X_test_plus_chosen)\n",
    "X_test_plus_chosen_FI = pd.get_dummies(X_test_plus_chosen_FI, columns=['site_name'])\n",
    "X_test_plus_chosen_FI = pd.get_dummies(X_test_plus_chosen_FI, columns=['counter_name'])\n",
    "X_test_plus_chosen_FI[continuous_columns] = scaler.fit_transform(X_test_plus_chosen_FI[continuous_columns])\n",
    "\n",
    "X_final_test_plus_chosen_FI = _encode_dates(X_final_test_plus_chosen)\n",
    "X_final_test_plus_chosen_FI = pd.get_dummies(X_final_test_plus_chosen_FI, columns=['site_name'])\n",
    "X_final_test_plus_chosen_FI = pd.get_dummies(X_final_test_plus_chosen_FI, columns=['counter_name'])\n",
    "X_final_test_plus_chosen_FI[continuous_columns] = scaler.fit_transform(X_final_test_plus_chosen_FI[continuous_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "da52225f-8e1b-4f99-8887-889af976d0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coucou\n",
      "Iteration 1, loss = 0.46534587\n",
      "Iteration 2, loss = 0.20989278\n",
      "Iteration 3, loss = 0.18261925\n",
      "Iteration 4, loss = 0.16417408\n",
      "Iteration 5, loss = 0.15216812\n",
      "Iteration 6, loss = 0.14426152\n",
      "Iteration 7, loss = 0.13890702\n",
      "Iteration 8, loss = 0.13427257\n",
      "Iteration 9, loss = 0.13100493\n",
      "Iteration 10, loss = 0.12772995\n",
      "Iteration 11, loss = 0.12498858\n",
      "Iteration 12, loss = 0.12268303\n",
      "Iteration 13, loss = 0.12082833\n",
      "Iteration 14, loss = 0.11898670\n",
      "Iteration 15, loss = 0.11756054\n",
      "Iteration 16, loss = 0.11610447\n",
      "Iteration 17, loss = 0.11462506\n",
      "Iteration 18, loss = 0.11360777\n",
      "Iteration 19, loss = 0.11259269\n",
      "Iteration 20, loss = 0.11167170\n",
      "Iteration 21, loss = 0.11099569\n",
      "Iteration 22, loss = 0.11016795\n",
      "Iteration 23, loss = 0.10923900\n",
      "Iteration 24, loss = 0.10870907\n",
      "Iteration 25, loss = 0.10791002\n",
      "Iteration 26, loss = 0.10734520\n",
      "Iteration 27, loss = 0.10679282\n",
      "Iteration 28, loss = 0.10633940\n",
      "Iteration 29, loss = 0.10590216\n",
      "Iteration 30, loss = 0.10550317\n",
      "Iteration 31, loss = 0.10480556\n",
      "Iteration 32, loss = 0.10437037\n",
      "Iteration 33, loss = 0.10438033\n",
      "Iteration 34, loss = 0.10356174\n",
      "Iteration 35, loss = 0.10340526\n",
      "Iteration 36, loss = 0.10285645\n",
      "Iteration 37, loss = 0.10259214\n",
      "Iteration 38, loss = 0.10213185\n",
      "Iteration 39, loss = 0.10188329\n",
      "Iteration 40, loss = 0.10141990\n",
      "Iteration 41, loss = 0.10116546\n",
      "Iteration 42, loss = 0.10095601\n",
      "Iteration 43, loss = 0.10042726\n",
      "Iteration 44, loss = 0.10018112\n",
      "Iteration 45, loss = 0.09992391\n",
      "Iteration 46, loss = 0.09987180\n",
      "Iteration 47, loss = 0.09960037\n",
      "Iteration 48, loss = 0.09910007\n",
      "Iteration 49, loss = 0.09885481\n",
      "Iteration 50, loss = 0.09873157\n",
      "Iteration 51, loss = 0.09846703\n",
      "Iteration 52, loss = 0.09838105\n",
      "Iteration 53, loss = 0.09791495\n",
      "Iteration 54, loss = 0.09803527\n",
      "Iteration 55, loss = 0.09755140\n",
      "Iteration 56, loss = 0.09759538\n",
      "Iteration 57, loss = 0.09720249\n",
      "Iteration 58, loss = 0.09727556\n",
      "Iteration 59, loss = 0.09691873\n",
      "Iteration 60, loss = 0.09662901\n",
      "Iteration 61, loss = 0.09662926\n",
      "Iteration 62, loss = 0.09644051\n",
      "Iteration 63, loss = 0.09633823\n",
      "Iteration 64, loss = 0.09625951\n",
      "Iteration 65, loss = 0.09609082\n",
      "Iteration 66, loss = 0.09604351\n",
      "Iteration 67, loss = 0.09592505\n",
      "Iteration 68, loss = 0.09572522\n",
      "Iteration 69, loss = 0.09557652\n",
      "Iteration 70, loss = 0.09533437\n",
      "Iteration 71, loss = 0.09537895\n",
      "Iteration 72, loss = 0.09525468\n",
      "Iteration 73, loss = 0.09520452\n",
      "Iteration 74, loss = 0.09488243\n",
      "Iteration 75, loss = 0.09498272\n",
      "Iteration 76, loss = 0.09483718\n",
      "Iteration 77, loss = 0.09471753\n",
      "Iteration 78, loss = 0.09466300\n",
      "Iteration 79, loss = 0.09449592\n",
      "Iteration 80, loss = 0.09455415\n",
      "Iteration 81, loss = 0.09440582\n",
      "Iteration 82, loss = 0.09430681\n",
      "Iteration 83, loss = 0.09428087\n",
      "Iteration 84, loss = 0.09411349\n",
      "Iteration 85, loss = 0.09414952\n",
      "Iteration 86, loss = 0.09409253\n",
      "Iteration 87, loss = 0.09389454\n",
      "Iteration 88, loss = 0.09389842\n",
      "Iteration 89, loss = 0.09389185\n",
      "Iteration 90, loss = 0.09389334\n",
      "Iteration 91, loss = 0.09378542\n",
      "Iteration 92, loss = 0.09369178\n",
      "Iteration 93, loss = 0.09365977\n",
      "Iteration 94, loss = 0.09355465\n",
      "Iteration 95, loss = 0.09357198\n",
      "Iteration 96, loss = 0.09349958\n",
      "Iteration 97, loss = 0.09323431\n",
      "Iteration 98, loss = 0.09342447\n",
      "Iteration 99, loss = 0.09336721\n",
      "Iteration 100, loss = 0.09316638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\envs\\bikes-count\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.44863191\n",
      "Iteration 2, loss = 0.20868476\n",
      "Iteration 3, loss = 0.18196965\n",
      "Iteration 4, loss = 0.16518282\n",
      "Iteration 5, loss = 0.15425714\n",
      "Iteration 6, loss = 0.14743985\n",
      "Iteration 7, loss = 0.14182717\n",
      "Iteration 8, loss = 0.13768581\n",
      "Iteration 9, loss = 0.13379651\n",
      "Iteration 10, loss = 0.13061385\n",
      "Iteration 11, loss = 0.12762843\n",
      "Iteration 12, loss = 0.12489099\n",
      "Iteration 13, loss = 0.12260546\n",
      "Iteration 14, loss = 0.12053087\n",
      "Iteration 15, loss = 0.11875621\n",
      "Iteration 16, loss = 0.11720312\n",
      "Iteration 17, loss = 0.11579729\n",
      "Iteration 18, loss = 0.11429756\n",
      "Iteration 19, loss = 0.11313609\n",
      "Iteration 20, loss = 0.11199613\n",
      "Iteration 21, loss = 0.11112764\n",
      "Iteration 22, loss = 0.10990574\n",
      "Iteration 23, loss = 0.10934489\n",
      "Iteration 24, loss = 0.10876020\n",
      "Iteration 25, loss = 0.10772394\n",
      "Iteration 26, loss = 0.10710631\n",
      "Iteration 27, loss = 0.10650468\n",
      "Iteration 28, loss = 0.10562910\n",
      "Iteration 29, loss = 0.10521290\n",
      "Iteration 30, loss = 0.10451419\n",
      "Iteration 31, loss = 0.10406719\n",
      "Iteration 32, loss = 0.10346219\n",
      "Iteration 33, loss = 0.10304085\n",
      "Iteration 34, loss = 0.10240745\n",
      "Iteration 35, loss = 0.10194096\n",
      "Iteration 36, loss = 0.10161949\n",
      "Iteration 37, loss = 0.10105967\n",
      "Iteration 38, loss = 0.10096952\n",
      "Iteration 39, loss = 0.10031180\n",
      "Iteration 40, loss = 0.10020604\n",
      "Iteration 41, loss = 0.09981389\n",
      "Iteration 42, loss = 0.09941872\n",
      "Iteration 43, loss = 0.09924379\n",
      "Iteration 44, loss = 0.09891741\n",
      "Iteration 45, loss = 0.09867457\n",
      "Iteration 46, loss = 0.09859513\n",
      "Iteration 47, loss = 0.09841941\n",
      "Iteration 48, loss = 0.09792732\n",
      "Iteration 49, loss = 0.09777257\n",
      "Iteration 50, loss = 0.09754218\n",
      "Iteration 51, loss = 0.09737583\n",
      "Iteration 52, loss = 0.09712508\n",
      "Iteration 53, loss = 0.09688387\n",
      "Iteration 54, loss = 0.09686853\n",
      "Iteration 55, loss = 0.09678165\n",
      "Iteration 56, loss = 0.09663721\n",
      "Iteration 57, loss = 0.09638940\n",
      "Iteration 58, loss = 0.09623295\n",
      "Iteration 59, loss = 0.09602905\n",
      "Iteration 60, loss = 0.09602281\n",
      "Iteration 61, loss = 0.09588426\n",
      "Iteration 62, loss = 0.09574866\n",
      "Iteration 63, loss = 0.09573987\n",
      "Iteration 64, loss = 0.09537753\n",
      "Iteration 65, loss = 0.09556159\n",
      "Iteration 66, loss = 0.09527940\n",
      "Iteration 67, loss = 0.09527773\n",
      "Iteration 68, loss = 0.09534037\n",
      "Iteration 69, loss = 0.09501774\n",
      "Iteration 70, loss = 0.09477963\n",
      "Iteration 71, loss = 0.09460345\n",
      "Iteration 72, loss = 0.09474226\n",
      "Iteration 73, loss = 0.09449645\n",
      "Iteration 74, loss = 0.09438457\n",
      "Iteration 75, loss = 0.09430014\n",
      "Iteration 76, loss = 0.09422728\n",
      "Iteration 77, loss = 0.09415562\n",
      "Iteration 78, loss = 0.09411007\n",
      "Iteration 79, loss = 0.09429707\n",
      "Iteration 80, loss = 0.09413126\n",
      "Iteration 81, loss = 0.09376422\n",
      "Iteration 82, loss = 0.09356721\n",
      "Iteration 83, loss = 0.09367803\n",
      "Iteration 84, loss = 0.09368529\n",
      "Iteration 85, loss = 0.09344319\n",
      "Iteration 86, loss = 0.09373248\n",
      "Iteration 87, loss = 0.09325660\n",
      "Iteration 88, loss = 0.09335768\n",
      "Iteration 89, loss = 0.09325329\n",
      "Iteration 90, loss = 0.09313411\n",
      "Iteration 91, loss = 0.09330998\n",
      "Iteration 92, loss = 0.09296828\n",
      "Iteration 93, loss = 0.09274313\n",
      "Iteration 94, loss = 0.09281124\n",
      "Iteration 95, loss = 0.09280834\n",
      "Iteration 96, loss = 0.09282715\n",
      "Iteration 97, loss = 0.09270338\n",
      "Iteration 98, loss = 0.09255311\n",
      "Iteration 99, loss = 0.09256861\n",
      "Iteration 100, loss = 0.09224586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\envs\\bikes-count\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.50557821\n",
      "Iteration 2, loss = 0.22271979\n",
      "Iteration 3, loss = 0.19668692\n",
      "Iteration 4, loss = 0.17794582\n",
      "Iteration 5, loss = 0.16429899\n",
      "Iteration 6, loss = 0.15420554\n",
      "Iteration 7, loss = 0.14680509\n",
      "Iteration 8, loss = 0.14081855\n",
      "Iteration 9, loss = 0.13616843\n",
      "Iteration 10, loss = 0.13234011\n",
      "Iteration 11, loss = 0.12890004\n",
      "Iteration 12, loss = 0.12587637\n",
      "Iteration 13, loss = 0.12340315\n",
      "Iteration 14, loss = 0.12128122\n",
      "Iteration 15, loss = 0.11927333\n",
      "Iteration 16, loss = 0.11752020\n",
      "Iteration 17, loss = 0.11587999\n",
      "Iteration 18, loss = 0.11422941\n",
      "Iteration 19, loss = 0.11311456\n",
      "Iteration 20, loss = 0.11189450\n",
      "Iteration 21, loss = 0.11099842\n",
      "Iteration 22, loss = 0.10995228\n",
      "Iteration 23, loss = 0.10933101\n",
      "Iteration 24, loss = 0.10844320\n",
      "Iteration 25, loss = 0.10768995\n",
      "Iteration 26, loss = 0.10707032\n",
      "Iteration 27, loss = 0.10644995\n",
      "Iteration 28, loss = 0.10601911\n",
      "Iteration 29, loss = 0.10543649\n",
      "Iteration 30, loss = 0.10486399\n",
      "Iteration 31, loss = 0.10462944\n",
      "Iteration 32, loss = 0.10448675\n",
      "Iteration 33, loss = 0.10405841\n",
      "Iteration 34, loss = 0.10375018\n",
      "Iteration 35, loss = 0.10331439\n",
      "Iteration 36, loss = 0.10295365\n",
      "Iteration 37, loss = 0.10273835\n",
      "Iteration 38, loss = 0.10252376\n",
      "Iteration 39, loss = 0.10192380\n",
      "Iteration 40, loss = 0.10177619\n",
      "Iteration 41, loss = 0.10169858\n",
      "Iteration 42, loss = 0.10136230\n",
      "Iteration 43, loss = 0.10089155\n",
      "Iteration 44, loss = 0.10084990\n",
      "Iteration 45, loss = 0.10081553\n",
      "Iteration 46, loss = 0.10032020\n",
      "Iteration 47, loss = 0.10031543\n",
      "Iteration 48, loss = 0.09992409\n",
      "Iteration 49, loss = 0.09972747\n",
      "Iteration 50, loss = 0.09963469\n",
      "Iteration 51, loss = 0.09936155\n",
      "Iteration 52, loss = 0.09925686\n",
      "Iteration 53, loss = 0.09906508\n",
      "Iteration 54, loss = 0.09893215\n",
      "Iteration 55, loss = 0.09860135\n",
      "Iteration 56, loss = 0.09858058\n",
      "Iteration 57, loss = 0.09824102\n",
      "Iteration 58, loss = 0.09828878\n",
      "Iteration 59, loss = 0.09803674\n",
      "Iteration 60, loss = 0.09801400\n",
      "Iteration 61, loss = 0.09788889\n",
      "Iteration 62, loss = 0.09760716\n",
      "Iteration 63, loss = 0.09756083\n",
      "Iteration 64, loss = 0.09729850\n",
      "Iteration 65, loss = 0.09732456\n",
      "Iteration 66, loss = 0.09713390\n",
      "Iteration 67, loss = 0.09709964\n",
      "Iteration 68, loss = 0.09674027\n",
      "Iteration 69, loss = 0.09690338\n",
      "Iteration 70, loss = 0.09684182\n",
      "Iteration 71, loss = 0.09672980\n",
      "Iteration 72, loss = 0.09631023\n",
      "Iteration 73, loss = 0.09637189\n",
      "Iteration 74, loss = 0.09600225\n",
      "Iteration 75, loss = 0.09621814\n",
      "Iteration 76, loss = 0.09627198\n",
      "Iteration 77, loss = 0.09604282\n",
      "Iteration 78, loss = 0.09597550\n",
      "Iteration 79, loss = 0.09585891\n",
      "Iteration 80, loss = 0.09556094\n",
      "Iteration 81, loss = 0.09580079\n",
      "Iteration 82, loss = 0.09532970\n",
      "Iteration 83, loss = 0.09543621\n",
      "Iteration 84, loss = 0.09550043\n",
      "Iteration 85, loss = 0.09528900\n",
      "Iteration 86, loss = 0.09512194\n",
      "Iteration 87, loss = 0.09506512\n",
      "Iteration 88, loss = 0.09504209\n",
      "Iteration 89, loss = 0.09475808\n",
      "Iteration 90, loss = 0.09475005\n",
      "Iteration 91, loss = 0.09467034\n",
      "Iteration 92, loss = 0.09468396\n",
      "Iteration 93, loss = 0.09488453\n",
      "Iteration 94, loss = 0.09451944\n",
      "Iteration 95, loss = 0.09461540\n",
      "Iteration 96, loss = 0.09452878\n",
      "Iteration 97, loss = 0.09423725\n",
      "Iteration 98, loss = 0.09428245\n",
      "Iteration 99, loss = 0.09419947\n",
      "Iteration 100, loss = 0.09411184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\envs\\bikes-count\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.46704150\n",
      "Iteration 2, loss = 0.21164737\n",
      "Iteration 3, loss = 0.18535022\n",
      "Iteration 4, loss = 0.16653494\n",
      "Iteration 5, loss = 0.15428639\n",
      "Iteration 6, loss = 0.14579910\n",
      "Iteration 7, loss = 0.13937330\n",
      "Iteration 8, loss = 0.13393887\n",
      "Iteration 9, loss = 0.12936184\n",
      "Iteration 10, loss = 0.12521869\n",
      "Iteration 11, loss = 0.12198472\n",
      "Iteration 12, loss = 0.11896741\n",
      "Iteration 13, loss = 0.11633238\n",
      "Iteration 14, loss = 0.11402214\n",
      "Iteration 15, loss = 0.11199229\n",
      "Iteration 16, loss = 0.11067670\n",
      "Iteration 17, loss = 0.10901053\n",
      "Iteration 18, loss = 0.10750275\n",
      "Iteration 19, loss = 0.10647039\n",
      "Iteration 20, loss = 0.10513344\n",
      "Iteration 21, loss = 0.10414505\n",
      "Iteration 22, loss = 0.10334782\n",
      "Iteration 23, loss = 0.10249764\n",
      "Iteration 24, loss = 0.10168295\n",
      "Iteration 25, loss = 0.10110639\n",
      "Iteration 26, loss = 0.10068082\n",
      "Iteration 27, loss = 0.09960838\n",
      "Iteration 28, loss = 0.09936923\n",
      "Iteration 29, loss = 0.09867485\n",
      "Iteration 30, loss = 0.09859074\n",
      "Iteration 31, loss = 0.09790299\n",
      "Iteration 32, loss = 0.09778064\n",
      "Iteration 33, loss = 0.09731472\n",
      "Iteration 34, loss = 0.09704924\n",
      "Iteration 35, loss = 0.09673011\n",
      "Iteration 36, loss = 0.09647098\n",
      "Iteration 37, loss = 0.09605478\n",
      "Iteration 38, loss = 0.09580787\n",
      "Iteration 39, loss = 0.09554322\n",
      "Iteration 40, loss = 0.09530233\n",
      "Iteration 41, loss = 0.09495686\n",
      "Iteration 42, loss = 0.09482703\n",
      "Iteration 43, loss = 0.09438523\n",
      "Iteration 44, loss = 0.09409366\n",
      "Iteration 45, loss = 0.09395954\n",
      "Iteration 46, loss = 0.09387598\n",
      "Iteration 47, loss = 0.09347836\n",
      "Iteration 48, loss = 0.09326993\n",
      "Iteration 49, loss = 0.09320906\n",
      "Iteration 50, loss = 0.09290280\n",
      "Iteration 51, loss = 0.09272384\n",
      "Iteration 52, loss = 0.09259880\n",
      "Iteration 53, loss = 0.09245309\n",
      "Iteration 54, loss = 0.09213817\n",
      "Iteration 55, loss = 0.09212419\n",
      "Iteration 56, loss = 0.09185258\n",
      "Iteration 57, loss = 0.09186400\n",
      "Iteration 58, loss = 0.09162823\n",
      "Iteration 59, loss = 0.09159861\n",
      "Iteration 60, loss = 0.09133975\n",
      "Iteration 61, loss = 0.09116783\n",
      "Iteration 62, loss = 0.09105110\n",
      "Iteration 63, loss = 0.09110127\n",
      "Iteration 64, loss = 0.09114163\n",
      "Iteration 65, loss = 0.09063524\n",
      "Iteration 66, loss = 0.09078533\n",
      "Iteration 67, loss = 0.09060227\n",
      "Iteration 68, loss = 0.09030776\n",
      "Iteration 69, loss = 0.09034258\n",
      "Iteration 70, loss = 0.09022596\n",
      "Iteration 71, loss = 0.09009652\n",
      "Iteration 72, loss = 0.08994819\n",
      "Iteration 73, loss = 0.08981118\n",
      "Iteration 74, loss = 0.08984391\n",
      "Iteration 75, loss = 0.08976852\n",
      "Iteration 76, loss = 0.08935137\n",
      "Iteration 77, loss = 0.08943488\n",
      "Iteration 78, loss = 0.08930234\n",
      "Iteration 79, loss = 0.08925127\n",
      "Iteration 80, loss = 0.08910125\n",
      "Iteration 81, loss = 0.08919604\n",
      "Iteration 82, loss = 0.08898963\n",
      "Iteration 83, loss = 0.08909166\n",
      "Iteration 84, loss = 0.08889345\n",
      "Iteration 85, loss = 0.08874726\n",
      "Iteration 86, loss = 0.08875227\n",
      "Iteration 87, loss = 0.08842743\n",
      "Iteration 88, loss = 0.08849592\n",
      "Iteration 89, loss = 0.08840185\n",
      "Iteration 90, loss = 0.08847223\n",
      "Iteration 91, loss = 0.08803431\n",
      "Iteration 92, loss = 0.08824368\n",
      "Iteration 93, loss = 0.08831513\n",
      "Iteration 94, loss = 0.08796543\n",
      "Iteration 95, loss = 0.08806693\n",
      "Iteration 96, loss = 0.08798205\n",
      "Iteration 97, loss = 0.08787784\n",
      "Iteration 98, loss = 0.08787366\n",
      "Iteration 99, loss = 0.08786389\n",
      "Iteration 100, loss = 0.08759339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\envs\\bikes-count\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.49767790\n",
      "Iteration 2, loss = 0.22078866\n",
      "Iteration 3, loss = 0.19231107\n",
      "Iteration 4, loss = 0.16994052\n",
      "Iteration 5, loss = 0.15538796\n",
      "Iteration 6, loss = 0.14620355\n",
      "Iteration 7, loss = 0.14000420\n",
      "Iteration 8, loss = 0.13468050\n",
      "Iteration 9, loss = 0.12993724\n",
      "Iteration 10, loss = 0.12590127\n",
      "Iteration 11, loss = 0.12291120\n",
      "Iteration 12, loss = 0.12035610\n",
      "Iteration 13, loss = 0.11834913\n",
      "Iteration 14, loss = 0.11656859\n",
      "Iteration 15, loss = 0.11493079\n",
      "Iteration 16, loss = 0.11355454\n",
      "Iteration 17, loss = 0.11246135\n",
      "Iteration 18, loss = 0.11165750\n",
      "Iteration 19, loss = 0.11033596\n",
      "Iteration 20, loss = 0.10966284\n",
      "Iteration 21, loss = 0.10880395\n",
      "Iteration 22, loss = 0.10803564\n",
      "Iteration 23, loss = 0.10741364\n",
      "Iteration 24, loss = 0.10682728\n",
      "Iteration 25, loss = 0.10609396\n",
      "Iteration 26, loss = 0.10552881\n",
      "Iteration 27, loss = 0.10516048\n",
      "Iteration 28, loss = 0.10472461\n",
      "Iteration 29, loss = 0.10402327\n",
      "Iteration 30, loss = 0.10375646\n",
      "Iteration 31, loss = 0.10350918\n",
      "Iteration 32, loss = 0.10303521\n",
      "Iteration 33, loss = 0.10267788\n",
      "Iteration 34, loss = 0.10228075\n",
      "Iteration 35, loss = 0.10206640\n",
      "Iteration 36, loss = 0.10176976\n",
      "Iteration 37, loss = 0.10134290\n",
      "Iteration 38, loss = 0.10112655\n",
      "Iteration 39, loss = 0.10086707\n",
      "Iteration 40, loss = 0.10069529\n",
      "Iteration 41, loss = 0.10033497\n",
      "Iteration 42, loss = 0.09988903\n",
      "Iteration 43, loss = 0.09966936\n",
      "Iteration 44, loss = 0.09933122\n",
      "Iteration 45, loss = 0.09925687\n",
      "Iteration 46, loss = 0.09878754\n",
      "Iteration 47, loss = 0.09890976\n",
      "Iteration 48, loss = 0.09862217\n",
      "Iteration 49, loss = 0.09846517\n",
      "Iteration 50, loss = 0.09803493\n",
      "Iteration 51, loss = 0.09802806\n",
      "Iteration 52, loss = 0.09782071\n",
      "Iteration 53, loss = 0.09772644\n",
      "Iteration 54, loss = 0.09746667\n",
      "Iteration 55, loss = 0.09723024\n",
      "Iteration 56, loss = 0.09721889\n",
      "Iteration 57, loss = 0.09680861\n",
      "Iteration 58, loss = 0.09667558\n",
      "Iteration 59, loss = 0.09692119\n",
      "Iteration 60, loss = 0.09669093\n",
      "Iteration 61, loss = 0.09658569\n",
      "Iteration 62, loss = 0.09626912\n",
      "Iteration 63, loss = 0.09624893\n",
      "Iteration 64, loss = 0.09615158\n",
      "Iteration 65, loss = 0.09595897\n",
      "Iteration 66, loss = 0.09592242\n",
      "Iteration 67, loss = 0.09584366\n",
      "Iteration 68, loss = 0.09586584\n",
      "Iteration 69, loss = 0.09568904\n",
      "Iteration 70, loss = 0.09537123\n",
      "Iteration 71, loss = 0.09556852\n",
      "Iteration 72, loss = 0.09523528\n",
      "Iteration 73, loss = 0.09512633\n",
      "Iteration 74, loss = 0.09504632\n",
      "Iteration 75, loss = 0.09484738\n",
      "Iteration 76, loss = 0.09489300\n",
      "Iteration 77, loss = 0.09477443\n",
      "Iteration 78, loss = 0.09478360\n",
      "Iteration 79, loss = 0.09466426\n",
      "Iteration 80, loss = 0.09464338\n",
      "Iteration 81, loss = 0.09447046\n",
      "Iteration 82, loss = 0.09436182\n",
      "Iteration 83, loss = 0.09439021\n",
      "Iteration 84, loss = 0.09432323\n",
      "Iteration 85, loss = 0.09432221\n",
      "Iteration 86, loss = 0.09428988\n",
      "Iteration 87, loss = 0.09412920\n",
      "Iteration 88, loss = 0.09401716\n",
      "Iteration 89, loss = 0.09408557\n",
      "Iteration 90, loss = 0.09381694\n",
      "Iteration 91, loss = 0.09375841\n",
      "Iteration 92, loss = 0.09377331\n",
      "Iteration 93, loss = 0.09360893\n",
      "Iteration 94, loss = 0.09352668\n",
      "Iteration 95, loss = 0.09357108\n",
      "Iteration 96, loss = 0.09354958\n",
      "Iteration 97, loss = 0.09339746\n",
      "Iteration 98, loss = 0.09340734\n",
      "Iteration 99, loss = 0.09314780\n",
      "Iteration 100, loss = 0.09306633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\envs\\bikes-count\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.47545469\n",
      "Iteration 2, loss = 0.20920951\n",
      "Iteration 3, loss = 0.18535607\n",
      "Iteration 4, loss = 0.16873304\n",
      "Iteration 5, loss = 0.15618220\n",
      "Iteration 6, loss = 0.14713759\n",
      "Iteration 7, loss = 0.14066096\n",
      "Iteration 8, loss = 0.13577346\n",
      "Iteration 9, loss = 0.13174162\n",
      "Iteration 10, loss = 0.12830739\n",
      "Iteration 11, loss = 0.12546157\n",
      "Iteration 12, loss = 0.12261560\n",
      "Iteration 13, loss = 0.12025958\n",
      "Iteration 14, loss = 0.11802267\n",
      "Iteration 15, loss = 0.11583342\n",
      "Iteration 16, loss = 0.11406760\n",
      "Iteration 17, loss = 0.11259005\n",
      "Iteration 18, loss = 0.11115122\n",
      "Iteration 19, loss = 0.10999388\n",
      "Iteration 20, loss = 0.10893936\n",
      "Iteration 21, loss = 0.10793739\n",
      "Iteration 22, loss = 0.10701378\n",
      "Iteration 23, loss = 0.10622783\n",
      "Iteration 24, loss = 0.10546332\n",
      "Iteration 25, loss = 0.10473400\n",
      "Iteration 26, loss = 0.10402575\n",
      "Iteration 27, loss = 0.10369589\n",
      "Iteration 28, loss = 0.10301705\n",
      "Iteration 29, loss = 0.10263224\n",
      "Iteration 30, loss = 0.10224360\n",
      "Iteration 31, loss = 0.10162464\n",
      "Iteration 32, loss = 0.10119398\n",
      "Iteration 33, loss = 0.10089040\n",
      "Iteration 34, loss = 0.10047508\n",
      "Iteration 35, loss = 0.10032925\n",
      "Iteration 36, loss = 0.09976395\n",
      "Iteration 37, loss = 0.09942393\n",
      "Iteration 38, loss = 0.09911962\n",
      "Iteration 39, loss = 0.09890749\n",
      "Iteration 40, loss = 0.09857034\n",
      "Iteration 41, loss = 0.09836667\n",
      "Iteration 42, loss = 0.09791071\n",
      "Iteration 43, loss = 0.09755262\n",
      "Iteration 44, loss = 0.09739023\n",
      "Iteration 45, loss = 0.09724984\n",
      "Iteration 46, loss = 0.09707111\n",
      "Iteration 47, loss = 0.09672603\n",
      "Iteration 48, loss = 0.09684070\n",
      "Iteration 49, loss = 0.09658127\n",
      "Iteration 50, loss = 0.09609344\n",
      "Iteration 51, loss = 0.09611892\n",
      "Iteration 52, loss = 0.09588374\n",
      "Iteration 53, loss = 0.09569697\n",
      "Iteration 54, loss = 0.09545274\n",
      "Iteration 55, loss = 0.09534483\n",
      "Iteration 56, loss = 0.09539018\n",
      "Iteration 57, loss = 0.09520702\n",
      "Iteration 58, loss = 0.09485131\n",
      "Iteration 59, loss = 0.09485141\n",
      "Iteration 60, loss = 0.09446009\n",
      "Iteration 61, loss = 0.09451041\n",
      "Iteration 62, loss = 0.09438270\n",
      "Iteration 63, loss = 0.09442332\n",
      "Iteration 64, loss = 0.09406133\n",
      "Iteration 65, loss = 0.09395525\n",
      "Iteration 66, loss = 0.09392575\n",
      "Iteration 67, loss = 0.09377712\n",
      "Iteration 68, loss = 0.09387711\n",
      "Iteration 69, loss = 0.09343292\n",
      "Iteration 70, loss = 0.09367335\n",
      "Iteration 71, loss = 0.09353922\n",
      "Iteration 72, loss = 0.09329454\n",
      "Iteration 73, loss = 0.09310744\n",
      "Iteration 74, loss = 0.09322785\n",
      "Iteration 75, loss = 0.09300610\n",
      "Iteration 76, loss = 0.09297374\n",
      "Iteration 77, loss = 0.09268998\n",
      "Iteration 78, loss = 0.09286894\n",
      "Iteration 79, loss = 0.09251490\n",
      "Iteration 80, loss = 0.09271738\n",
      "Iteration 81, loss = 0.09246641\n",
      "Iteration 82, loss = 0.09257957\n",
      "Iteration 83, loss = 0.09242308\n",
      "Iteration 84, loss = 0.09247686\n",
      "Iteration 85, loss = 0.09225699\n",
      "Iteration 86, loss = 0.09211754\n",
      "Iteration 87, loss = 0.09213500\n",
      "Iteration 88, loss = 0.09194790\n",
      "Iteration 89, loss = 0.09197443\n",
      "Iteration 90, loss = 0.09195679\n",
      "Iteration 91, loss = 0.09190062\n",
      "Iteration 92, loss = 0.09156609\n",
      "Iteration 93, loss = 0.09159574\n",
      "Iteration 94, loss = 0.09182454\n",
      "Iteration 95, loss = 0.09175391\n",
      "Iteration 96, loss = 0.09142746\n",
      "Iteration 97, loss = 0.09141183\n",
      "Iteration 98, loss = 0.09154270\n",
      "Iteration 99, loss = 0.09132971\n",
      "Iteration 100, loss = 0.09135573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\envs\\bikes-count\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.96123051\n",
      "Iteration 2, loss = 0.49096385\n",
      "Iteration 3, loss = 0.32583930\n",
      "Iteration 4, loss = 0.23995191\n",
      "Iteration 5, loss = 0.20771447\n",
      "Iteration 6, loss = 0.19420650\n",
      "Iteration 7, loss = 0.18329820\n",
      "Iteration 8, loss = 0.17445178\n",
      "Iteration 9, loss = 0.16684471\n",
      "Iteration 10, loss = 0.16110348\n",
      "Iteration 11, loss = 0.15440349\n",
      "Iteration 12, loss = 0.14880410\n",
      "Iteration 13, loss = 0.14395039\n",
      "Iteration 14, loss = 0.13842588\n",
      "Iteration 15, loss = 0.13399489\n",
      "Iteration 16, loss = 0.12936010\n",
      "Iteration 17, loss = 0.12548281\n",
      "Iteration 18, loss = 0.12165266\n",
      "Iteration 19, loss = 0.11750780\n",
      "Iteration 20, loss = 0.11529621\n",
      "Iteration 21, loss = 0.11186870\n",
      "Iteration 22, loss = 0.10943395\n",
      "Iteration 23, loss = 0.10708887\n",
      "Iteration 24, loss = 0.10464441\n",
      "Iteration 25, loss = 0.10276013\n",
      "Iteration 26, loss = 0.10070447\n",
      "Iteration 27, loss = 0.09904495\n",
      "Iteration 28, loss = 0.09713869\n",
      "Iteration 29, loss = 0.09577397\n",
      "Iteration 30, loss = 0.09443994\n",
      "Iteration 31, loss = 0.09238137\n",
      "Iteration 32, loss = 0.09155788\n",
      "Iteration 33, loss = 0.08980312\n",
      "Iteration 34, loss = 0.08874903\n",
      "Iteration 35, loss = 0.08715392\n",
      "Iteration 36, loss = 0.08608760\n",
      "Iteration 37, loss = 0.08521249\n",
      "Iteration 38, loss = 0.08404114\n",
      "Iteration 39, loss = 0.08302839\n",
      "Iteration 40, loss = 0.08189151\n",
      "Iteration 41, loss = 0.08088548\n",
      "Iteration 42, loss = 0.08046400\n",
      "Iteration 43, loss = 0.07903393\n",
      "Iteration 44, loss = 0.07884408\n",
      "Iteration 45, loss = 0.07780026\n",
      "Iteration 46, loss = 0.07673688\n",
      "Iteration 47, loss = 0.07619295\n",
      "Iteration 48, loss = 0.07564498\n",
      "Iteration 49, loss = 0.07519802\n",
      "Iteration 50, loss = 0.07430342\n",
      "Iteration 51, loss = 0.07348564\n",
      "Iteration 52, loss = 0.07261253\n",
      "Iteration 53, loss = 0.07253692\n",
      "Iteration 54, loss = 0.07141628\n",
      "Iteration 55, loss = 0.07129625\n",
      "Iteration 56, loss = 0.07026721\n",
      "Iteration 57, loss = 0.07044141\n",
      "Iteration 58, loss = 0.06958809\n",
      "Iteration 59, loss = 0.06900397\n",
      "Iteration 60, loss = 0.06848898\n",
      "Iteration 61, loss = 0.06812670\n",
      "Iteration 62, loss = 0.06756068\n",
      "Iteration 63, loss = 0.06671096\n",
      "Iteration 64, loss = 0.06678407\n",
      "Iteration 65, loss = 0.06662932\n",
      "Iteration 66, loss = 0.06617786\n",
      "Iteration 67, loss = 0.06555132\n",
      "Iteration 68, loss = 0.06543605\n",
      "Iteration 69, loss = 0.06472012\n",
      "Iteration 70, loss = 0.06447631\n",
      "Iteration 71, loss = 0.06402494\n",
      "Iteration 72, loss = 0.06358565\n",
      "Iteration 73, loss = 0.06352301\n",
      "Iteration 74, loss = 0.06291786\n",
      "Iteration 75, loss = 0.06303317\n",
      "Iteration 76, loss = 0.06226634\n",
      "Iteration 77, loss = 0.06202672\n",
      "Iteration 78, loss = 0.06194310\n",
      "Iteration 79, loss = 0.06158148\n",
      "Iteration 80, loss = 0.06128358\n",
      "Iteration 81, loss = 0.06090220\n",
      "Iteration 82, loss = 0.06061662\n",
      "Iteration 83, loss = 0.06024281\n",
      "Iteration 84, loss = 0.06001746\n",
      "Iteration 85, loss = 0.05998795\n",
      "Iteration 86, loss = 0.05950239\n",
      "Iteration 87, loss = 0.05942953\n",
      "Iteration 88, loss = 0.05899175\n",
      "Iteration 89, loss = 0.05913029\n",
      "Iteration 90, loss = 0.05872950\n",
      "Iteration 91, loss = 0.05789725\n",
      "Iteration 92, loss = 0.05815386\n",
      "Iteration 93, loss = 0.05776962\n",
      "Iteration 94, loss = 0.05789077\n",
      "Iteration 95, loss = 0.05728238\n",
      "Iteration 96, loss = 0.05765894\n",
      "Iteration 97, loss = 0.05762111\n",
      "Iteration 98, loss = 0.05766820\n",
      "Iteration 99, loss = 0.05682849\n",
      "Iteration 100, loss = 0.05711152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\envs\\bikes-count\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.25180996\n",
      "Iteration 2, loss = 0.44203853\n",
      "Iteration 3, loss = 0.29243708\n",
      "Iteration 4, loss = 0.21883664\n",
      "Iteration 5, loss = 0.19055284\n",
      "Iteration 6, loss = 0.17650656\n",
      "Iteration 7, loss = 0.16762830\n",
      "Iteration 8, loss = 0.16068282\n",
      "Iteration 9, loss = 0.15456793\n",
      "Iteration 10, loss = 0.15029877\n",
      "Iteration 11, loss = 0.14588400\n",
      "Iteration 12, loss = 0.14180054\n",
      "Iteration 13, loss = 0.13859441\n",
      "Iteration 14, loss = 0.13542278\n",
      "Iteration 15, loss = 0.13343621\n",
      "Iteration 16, loss = 0.13001174\n",
      "Iteration 17, loss = 0.12838113\n",
      "Iteration 18, loss = 0.12634327\n",
      "Iteration 19, loss = 0.12390895\n",
      "Iteration 20, loss = 0.12249144\n",
      "Iteration 21, loss = 0.12030872\n",
      "Iteration 22, loss = 0.11861044\n",
      "Iteration 23, loss = 0.11699472\n",
      "Iteration 24, loss = 0.11508947\n",
      "Iteration 25, loss = 0.11283482\n",
      "Iteration 26, loss = 0.11122868\n",
      "Iteration 27, loss = 0.10964259\n",
      "Iteration 28, loss = 0.10804424\n",
      "Iteration 29, loss = 0.10610832\n",
      "Iteration 30, loss = 0.10499604\n",
      "Iteration 31, loss = 0.10289625\n",
      "Iteration 32, loss = 0.10101848\n",
      "Iteration 33, loss = 0.09964199\n",
      "Iteration 34, loss = 0.09791162\n",
      "Iteration 35, loss = 0.09656258\n",
      "Iteration 36, loss = 0.09542224\n",
      "Iteration 37, loss = 0.09395267\n",
      "Iteration 38, loss = 0.09265583\n",
      "Iteration 39, loss = 0.09176350\n",
      "Iteration 40, loss = 0.09106621\n",
      "Iteration 41, loss = 0.08914118\n",
      "Iteration 42, loss = 0.08877773\n",
      "Iteration 43, loss = 0.08739447\n",
      "Iteration 44, loss = 0.08661818\n",
      "Iteration 45, loss = 0.08592366\n",
      "Iteration 46, loss = 0.08458727\n",
      "Iteration 47, loss = 0.08366080\n",
      "Iteration 48, loss = 0.08315642\n",
      "Iteration 49, loss = 0.08220718\n",
      "Iteration 50, loss = 0.08141218\n",
      "Iteration 51, loss = 0.08032567\n",
      "Iteration 52, loss = 0.07995068\n",
      "Iteration 53, loss = 0.07949295\n",
      "Iteration 54, loss = 0.07858374\n",
      "Iteration 55, loss = 0.07774110\n",
      "Iteration 56, loss = 0.07746354\n",
      "Iteration 57, loss = 0.07639908\n",
      "Iteration 58, loss = 0.07577036\n",
      "Iteration 59, loss = 0.07571966\n",
      "Iteration 60, loss = 0.07522266\n",
      "Iteration 61, loss = 0.07497725\n",
      "Iteration 62, loss = 0.07387309\n",
      "Iteration 63, loss = 0.07349869\n",
      "Iteration 64, loss = 0.07303530\n",
      "Iteration 65, loss = 0.07281263\n",
      "Iteration 66, loss = 0.07254110\n",
      "Iteration 67, loss = 0.07177470\n",
      "Iteration 68, loss = 0.07075218\n",
      "Iteration 69, loss = 0.07082651\n",
      "Iteration 70, loss = 0.07000128\n",
      "Iteration 71, loss = 0.06979077\n",
      "Iteration 72, loss = 0.06955246\n",
      "Iteration 73, loss = 0.06907876\n",
      "Iteration 74, loss = 0.06893400\n",
      "Iteration 75, loss = 0.06827551\n",
      "Iteration 76, loss = 0.06773013\n",
      "Iteration 77, loss = 0.06757306\n",
      "Iteration 78, loss = 0.06704416\n",
      "Iteration 79, loss = 0.06686857\n",
      "Iteration 80, loss = 0.06672777\n",
      "Iteration 81, loss = 0.06635863\n",
      "Iteration 82, loss = 0.06594225\n",
      "Iteration 83, loss = 0.06526566\n",
      "Iteration 84, loss = 0.06523596\n",
      "Iteration 85, loss = 0.06473421\n",
      "Iteration 86, loss = 0.06459046\n",
      "Iteration 87, loss = 0.06428070\n",
      "Iteration 88, loss = 0.06421896\n",
      "Iteration 89, loss = 0.06385037\n",
      "Iteration 90, loss = 0.06327417\n",
      "Iteration 91, loss = 0.06305556\n",
      "Iteration 92, loss = 0.06294435\n",
      "Iteration 93, loss = 0.06255492\n",
      "Iteration 94, loss = 0.06257490\n",
      "Iteration 95, loss = 0.06264400\n",
      "Iteration 96, loss = 0.06169062\n",
      "Iteration 97, loss = 0.06196339\n",
      "Iteration 98, loss = 0.06174041\n",
      "Iteration 99, loss = 0.06153384\n",
      "Iteration 100, loss = 0.06113210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\envs\\bikes-count\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.40105457\n",
      "Iteration 2, loss = 0.47279395\n",
      "Iteration 3, loss = 0.32706558\n",
      "Iteration 4, loss = 0.23850318\n",
      "Iteration 5, loss = 0.20320399\n",
      "Iteration 6, loss = 0.18740700\n",
      "Iteration 7, loss = 0.17593348\n",
      "Iteration 8, loss = 0.16682112\n",
      "Iteration 9, loss = 0.16042568\n",
      "Iteration 10, loss = 0.15365549\n",
      "Iteration 11, loss = 0.14859067\n",
      "Iteration 12, loss = 0.14420250\n",
      "Iteration 13, loss = 0.14006849\n",
      "Iteration 14, loss = 0.13679342\n",
      "Iteration 15, loss = 0.13307050\n",
      "Iteration 16, loss = 0.12993141\n",
      "Iteration 17, loss = 0.12703335\n",
      "Iteration 18, loss = 0.12444375\n",
      "Iteration 19, loss = 0.12131796\n",
      "Iteration 20, loss = 0.11876816\n",
      "Iteration 21, loss = 0.11688454\n",
      "Iteration 22, loss = 0.11436839\n",
      "Iteration 23, loss = 0.11197245\n",
      "Iteration 24, loss = 0.11011338\n",
      "Iteration 25, loss = 0.10774460\n",
      "Iteration 26, loss = 0.10626535\n",
      "Iteration 27, loss = 0.10444568\n",
      "Iteration 28, loss = 0.10269956\n",
      "Iteration 29, loss = 0.10156349\n",
      "Iteration 30, loss = 0.09959090\n",
      "Iteration 31, loss = 0.09838668\n",
      "Iteration 32, loss = 0.09701173\n",
      "Iteration 33, loss = 0.09529178\n",
      "Iteration 34, loss = 0.09440236\n",
      "Iteration 35, loss = 0.09317420\n",
      "Iteration 36, loss = 0.09231889\n",
      "Iteration 37, loss = 0.09133911\n",
      "Iteration 38, loss = 0.09062576\n",
      "Iteration 39, loss = 0.08889087\n",
      "Iteration 40, loss = 0.08824528\n",
      "Iteration 41, loss = 0.08752568\n",
      "Iteration 42, loss = 0.08698839\n",
      "Iteration 43, loss = 0.08565921\n",
      "Iteration 44, loss = 0.08555318\n",
      "Iteration 45, loss = 0.08410855\n",
      "Iteration 46, loss = 0.08367929\n",
      "Iteration 47, loss = 0.08330407\n",
      "Iteration 48, loss = 0.08257455\n",
      "Iteration 49, loss = 0.08122882\n",
      "Iteration 50, loss = 0.08087350\n",
      "Iteration 51, loss = 0.07985660\n",
      "Iteration 52, loss = 0.07988184\n",
      "Iteration 53, loss = 0.07951012\n",
      "Iteration 54, loss = 0.07842141\n",
      "Iteration 55, loss = 0.07789333\n",
      "Iteration 56, loss = 0.07774856\n",
      "Iteration 57, loss = 0.07716702\n",
      "Iteration 58, loss = 0.07637313\n",
      "Iteration 59, loss = 0.07589285\n",
      "Iteration 60, loss = 0.07523613\n",
      "Iteration 61, loss = 0.07476365\n",
      "Iteration 62, loss = 0.07441630\n",
      "Iteration 63, loss = 0.07384943\n",
      "Iteration 64, loss = 0.07382260\n",
      "Iteration 65, loss = 0.07272848\n",
      "Iteration 66, loss = 0.07248903\n",
      "Iteration 67, loss = 0.07213856\n",
      "Iteration 68, loss = 0.07198029\n",
      "Iteration 69, loss = 0.07162992\n",
      "Iteration 70, loss = 0.07102558\n",
      "Iteration 71, loss = 0.07063296\n",
      "Iteration 72, loss = 0.07035457\n",
      "Iteration 73, loss = 0.07014524\n",
      "Iteration 74, loss = 0.06956548\n",
      "Iteration 75, loss = 0.06904361\n",
      "Iteration 76, loss = 0.06914478\n",
      "Iteration 77, loss = 0.06864290\n",
      "Iteration 78, loss = 0.06860757\n",
      "Iteration 79, loss = 0.06878536\n",
      "Iteration 80, loss = 0.06834561\n",
      "Iteration 81, loss = 0.06763998\n",
      "Iteration 82, loss = 0.06745642\n",
      "Iteration 83, loss = 0.06690638\n",
      "Iteration 84, loss = 0.06687300\n",
      "Iteration 85, loss = 0.06648245\n",
      "Iteration 86, loss = 0.06672824\n",
      "Iteration 87, loss = 0.06675157\n",
      "Iteration 88, loss = 0.06646903\n",
      "Iteration 89, loss = 0.06578183\n",
      "Iteration 90, loss = 0.06577157\n",
      "Iteration 91, loss = 0.06531127\n",
      "Iteration 92, loss = 0.06503386\n",
      "Iteration 93, loss = 0.06510281\n",
      "Iteration 94, loss = 0.06465047\n",
      "Iteration 95, loss = 0.06452234\n",
      "Iteration 96, loss = 0.06478594\n",
      "Iteration 97, loss = 0.06405496\n",
      "Iteration 98, loss = 0.06378632\n",
      "Iteration 99, loss = 0.06367302\n",
      "Iteration 100, loss = 0.06383101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\envs\\bikes-count\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.60274373\n",
      "Iteration 2, loss = 0.46848293\n",
      "Iteration 3, loss = 0.32066118\n",
      "Iteration 4, loss = 0.22950910\n",
      "Iteration 5, loss = 0.19458687\n",
      "Iteration 6, loss = 0.17854060\n",
      "Iteration 7, loss = 0.16728285\n",
      "Iteration 8, loss = 0.15889555\n",
      "Iteration 9, loss = 0.15210569\n",
      "Iteration 10, loss = 0.14569998\n",
      "Iteration 11, loss = 0.14020946\n",
      "Iteration 12, loss = 0.13578084\n",
      "Iteration 13, loss = 0.13280818\n",
      "Iteration 14, loss = 0.12915375\n",
      "Iteration 15, loss = 0.12602970\n",
      "Iteration 16, loss = 0.12427628\n",
      "Iteration 17, loss = 0.12145964\n",
      "Iteration 18, loss = 0.11954986\n",
      "Iteration 19, loss = 0.11754256\n",
      "Iteration 20, loss = 0.11558108\n",
      "Iteration 21, loss = 0.11366257\n",
      "Iteration 22, loss = 0.11226561\n",
      "Iteration 23, loss = 0.11098284\n",
      "Iteration 24, loss = 0.10956515\n",
      "Iteration 25, loss = 0.10806533\n",
      "Iteration 26, loss = 0.10654284\n",
      "Iteration 27, loss = 0.10568155\n",
      "Iteration 28, loss = 0.10393200\n",
      "Iteration 29, loss = 0.10297014\n",
      "Iteration 30, loss = 0.10196054\n",
      "Iteration 31, loss = 0.10043413\n",
      "Iteration 32, loss = 0.09956754\n",
      "Iteration 33, loss = 0.09814827\n",
      "Iteration 34, loss = 0.09725950\n",
      "Iteration 35, loss = 0.09591093\n",
      "Iteration 36, loss = 0.09513885\n",
      "Iteration 37, loss = 0.09430164\n",
      "Iteration 38, loss = 0.09271825\n",
      "Iteration 39, loss = 0.09251931\n",
      "Iteration 40, loss = 0.09203457\n",
      "Iteration 41, loss = 0.09056855\n",
      "Iteration 42, loss = 0.09003080\n",
      "Iteration 43, loss = 0.08933756\n",
      "Iteration 44, loss = 0.08879902\n",
      "Iteration 45, loss = 0.08784318\n",
      "Iteration 46, loss = 0.08709828\n",
      "Iteration 47, loss = 0.08667394\n",
      "Iteration 48, loss = 0.08598827\n",
      "Iteration 49, loss = 0.08557915\n",
      "Iteration 50, loss = 0.08485410\n",
      "Iteration 51, loss = 0.08438599\n",
      "Iteration 52, loss = 0.08371311\n",
      "Iteration 53, loss = 0.08304696\n",
      "Iteration 54, loss = 0.08302897\n",
      "Iteration 55, loss = 0.08205809\n",
      "Iteration 56, loss = 0.08214671\n",
      "Iteration 57, loss = 0.08113257\n",
      "Iteration 58, loss = 0.08103224\n",
      "Iteration 59, loss = 0.08000688\n",
      "Iteration 60, loss = 0.07961722\n",
      "Iteration 61, loss = 0.07905664\n",
      "Iteration 62, loss = 0.07836595\n",
      "Iteration 63, loss = 0.07914733\n",
      "Iteration 64, loss = 0.07766618\n",
      "Iteration 65, loss = 0.07756886\n",
      "Iteration 66, loss = 0.07706085\n",
      "Iteration 67, loss = 0.07733441\n",
      "Iteration 68, loss = 0.07649778\n",
      "Iteration 69, loss = 0.07627336\n",
      "Iteration 70, loss = 0.07571217\n",
      "Iteration 71, loss = 0.07519757\n",
      "Iteration 72, loss = 0.07497838\n",
      "Iteration 73, loss = 0.07458850\n",
      "Iteration 74, loss = 0.07470694\n",
      "Iteration 75, loss = 0.07382233\n",
      "Iteration 76, loss = 0.07359413\n",
      "Iteration 77, loss = 0.07303441\n",
      "Iteration 78, loss = 0.07314888\n",
      "Iteration 79, loss = 0.07277063\n",
      "Iteration 80, loss = 0.07216428\n",
      "Iteration 81, loss = 0.07214066\n",
      "Iteration 82, loss = 0.07187958\n",
      "Iteration 83, loss = 0.07146955\n",
      "Iteration 84, loss = 0.07168487\n",
      "Iteration 85, loss = 0.07109704\n",
      "Iteration 86, loss = 0.07057631\n",
      "Iteration 87, loss = 0.06987601\n",
      "Iteration 88, loss = 0.07033811\n",
      "Iteration 89, loss = 0.06961935\n",
      "Iteration 90, loss = 0.06929776\n",
      "Iteration 91, loss = 0.06939184\n",
      "Iteration 92, loss = 0.06875379\n",
      "Iteration 93, loss = 0.06818980\n",
      "Iteration 94, loss = 0.06805028\n",
      "Iteration 95, loss = 0.06772180\n",
      "Iteration 96, loss = 0.06788177\n",
      "Iteration 97, loss = 0.06787812\n",
      "Iteration 98, loss = 0.06740706\n",
      "Iteration 99, loss = 0.06735359\n",
      "Iteration 100, loss = 0.06651721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\envs\\bikes-count\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.98495201\n",
      "Iteration 2, loss = 0.46657537\n",
      "Iteration 3, loss = 0.30683155\n",
      "Iteration 4, loss = 0.21361748\n",
      "Iteration 5, loss = 0.18256172\n",
      "Iteration 6, loss = 0.16676454\n",
      "Iteration 7, loss = 0.15465920\n",
      "Iteration 8, loss = 0.14540333\n",
      "Iteration 9, loss = 0.13796548\n",
      "Iteration 10, loss = 0.13219449\n",
      "Iteration 11, loss = 0.12690075\n",
      "Iteration 12, loss = 0.12271509\n",
      "Iteration 13, loss = 0.11928543\n",
      "Iteration 14, loss = 0.11620177\n",
      "Iteration 15, loss = 0.11335561\n",
      "Iteration 16, loss = 0.11042621\n",
      "Iteration 17, loss = 0.10847620\n",
      "Iteration 18, loss = 0.10640525\n",
      "Iteration 19, loss = 0.10422735\n",
      "Iteration 20, loss = 0.10255572\n",
      "Iteration 21, loss = 0.10105539\n",
      "Iteration 22, loss = 0.09988517\n",
      "Iteration 23, loss = 0.09860085\n",
      "Iteration 24, loss = 0.09700264\n",
      "Iteration 25, loss = 0.09589907\n",
      "Iteration 26, loss = 0.09477276\n",
      "Iteration 27, loss = 0.09375813\n",
      "Iteration 28, loss = 0.09235423\n",
      "Iteration 29, loss = 0.09127020\n",
      "Iteration 30, loss = 0.08985153\n",
      "Iteration 31, loss = 0.08945555\n",
      "Iteration 32, loss = 0.08843605\n",
      "Iteration 33, loss = 0.08719255\n",
      "Iteration 34, loss = 0.08620123\n",
      "Iteration 35, loss = 0.08542342\n",
      "Iteration 36, loss = 0.08439684\n",
      "Iteration 37, loss = 0.08405518\n",
      "Iteration 38, loss = 0.08333660\n",
      "Iteration 39, loss = 0.08217651\n",
      "Iteration 40, loss = 0.08166656\n",
      "Iteration 41, loss = 0.08076086\n",
      "Iteration 42, loss = 0.07970197\n",
      "Iteration 43, loss = 0.07878393\n",
      "Iteration 44, loss = 0.07866615\n",
      "Iteration 45, loss = 0.07784518\n",
      "Iteration 46, loss = 0.07733884\n",
      "Iteration 47, loss = 0.07705094\n",
      "Iteration 48, loss = 0.07610231\n",
      "Iteration 49, loss = 0.07578498\n",
      "Iteration 50, loss = 0.07476837\n",
      "Iteration 51, loss = 0.07445469\n",
      "Iteration 52, loss = 0.07443692\n",
      "Iteration 53, loss = 0.07356589\n",
      "Iteration 54, loss = 0.07318705\n",
      "Iteration 55, loss = 0.07280631\n",
      "Iteration 56, loss = 0.07225295\n",
      "Iteration 57, loss = 0.07181635\n",
      "Iteration 58, loss = 0.07149958\n",
      "Iteration 59, loss = 0.07093724\n",
      "Iteration 60, loss = 0.07094457\n",
      "Iteration 61, loss = 0.07081180\n",
      "Iteration 62, loss = 0.07076128\n",
      "Iteration 63, loss = 0.07004043\n",
      "Iteration 64, loss = 0.06951197\n",
      "Iteration 65, loss = 0.06926524\n",
      "Iteration 66, loss = 0.06917524\n",
      "Iteration 67, loss = 0.06869227\n",
      "Iteration 68, loss = 0.06827418\n",
      "Iteration 69, loss = 0.06821740\n",
      "Iteration 70, loss = 0.06762176\n",
      "Iteration 71, loss = 0.06731362\n",
      "Iteration 72, loss = 0.06731673\n",
      "Iteration 73, loss = 0.06721566\n",
      "Iteration 74, loss = 0.06754192\n",
      "Iteration 75, loss = 0.06655781\n",
      "Iteration 76, loss = 0.06622149\n",
      "Iteration 77, loss = 0.06624069\n",
      "Iteration 78, loss = 0.06585597\n",
      "Iteration 79, loss = 0.06605872\n",
      "Iteration 80, loss = 0.06519116\n",
      "Iteration 81, loss = 0.06505237\n",
      "Iteration 82, loss = 0.06524057\n",
      "Iteration 83, loss = 0.06512729\n",
      "Iteration 84, loss = 0.06515890\n",
      "Iteration 85, loss = 0.06457512\n",
      "Iteration 86, loss = 0.06468843\n",
      "Iteration 87, loss = 0.06399662\n",
      "Iteration 88, loss = 0.06409899\n",
      "Iteration 89, loss = 0.06368174\n",
      "Iteration 90, loss = 0.06378807\n",
      "Iteration 91, loss = 0.06401825\n",
      "Iteration 92, loss = 0.06333526\n",
      "Iteration 93, loss = 0.06325161\n",
      "Iteration 94, loss = 0.06290538\n",
      "Iteration 95, loss = 0.06307505\n",
      "Iteration 96, loss = 0.06278966\n",
      "Iteration 97, loss = 0.06239523\n",
      "Iteration 98, loss = 0.06251101\n",
      "Iteration 99, loss = 0.06200630\n",
      "Iteration 100, loss = 0.06261994\n",
      "Train set, RMSE=1.36\n",
      "Test set, RMSE=0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\envs\\bikes-count\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LGMBRegressor\n",
    "Regressor = lgb.LGBMRegressor()\n",
    "\n",
    "# Ridge\n",
    "#Regressor = Ridge()\n",
    "\n",
    "# Initialize the XGBRegressor\n",
    "#Regressor = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Initialize the MLPRegressor\n",
    "Regressor = MLPRegressor(hidden_layer_sizes=(100,), \n",
    "                         activation='relu', \n",
    "                         solver='adam', \n",
    "                         alpha=0.0001, \n",
    "                         batch_size='auto', \n",
    "                         learning_rate='constant', \n",
    "                         learning_rate_init=0.001, \n",
    "                         max_iter=100, \n",
    "                         shuffle=True, \n",
    "                         random_state=None, \n",
    "                         tol=0.0001, \n",
    "                         verbose=True, \n",
    "                         warm_start=True\n",
    "                         )\n",
    "\n",
    "print(\"coucou\")\n",
    "# Train model with selected features\n",
    "Regressor.fit(X_train_plus_chosen_FI, y_train)\n",
    "\n",
    "get_RMSE_local_wt_pipe(Regressor, X_train_plus_chosen_FI, y_train, X_test_plus_chosen_FI, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5c1e0-744d-4854-8b1e-a2a3904357ab",
   "metadata": {},
   "source": [
    "### Final csv file with current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec820acb-5084-43e7-9ca5-ec650af1fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.33023798 2.22080372 1.70775034 ... 2.07673133 1.39920118 1.48307387]\n"
     ]
    }
   ],
   "source": [
    "# Get submission kaggle to csv\n",
    "submission_kaggle(Regressor, X_final_test_plus_chosen_FI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecf8783-4d94-4e39-9855-ddb92bb52a68",
   "metadata": {},
   "source": [
    "### Feature selections with RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c28e9d-20e5-45f9-a805-1b7cefd8156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Initialize the XGBRegressor\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Initialize RFECV\n",
    "selector = RFECV(estimator=xgb_reg, step=1, cv=5)\n",
    "\n",
    "# Fit RFECV\n",
    "selector = selector.fit(X_train_plus_FI, y_train)\n",
    "\n",
    "# Print the optimal number of features\n",
    "print(\"Optimal number of features : %d\" % selector.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fad72-978b-4a20-aed6-ab00bfe55c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print selected features\n",
    "selected_features = [feature for feature, selected in zip(X_train_plus_FI.columns, selector.support_) if selected]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a115a3-9de1-4f58-b4f6-1fc03559ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training and testing sets\n",
    "X_train_selected = selector.transform(X_train_plus_FI)\n",
    "X_test_selected = selector.transform(X_test_plus_FI)\n",
    "\n",
    "# Train model with selected features\n",
    "xgb_reg.fit(X_train_selected, y_train)\n",
    "\n",
    "get_RMSE_local(xgb_reg, X_train_selected, y_train, X_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba8129-c0a4-4f9f-bbd0-f34ac11032f7",
   "metadata": {},
   "source": [
    "## Train with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e2f20-470c-4b16-b5f8-fc3fcaef4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_train):\n",
    "    \n",
    "    date_encoder = FunctionTransformer(_encode_dates)\n",
    "    date_cols = _encode_dates(X_train[[\"date\"]]).columns.tolist()\n",
    "\n",
    "    categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    categorical_cols = [\"counter_name\", \"site_name\"]\n",
    "\n",
    "    numeric_encoder = StandardScaler()\n",
    "    numeric_cols = ['latitude', 'longitude', 't', 'ff', 'u', 'ssfrai', 'n', 'vv', 'rr3', 'hosp', 'rea', 'incid_rea', 'rad', 'Count_accidents']\n",
    "    numeric_cols = ['t', 'ff', 'u', 'ssfrai', 'n', 'vv', 'rr3', 'hosp', 'rea', 'incid_rea', 'rad', 'Count_accidents']\n",
    "    #numeric_cols = ['t', 'ff', 'u', 'ssfrai', 'n', 'vv', 'rr3']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"date\", OneHotEncoder(handle_unknown=\"ignore\"), date_cols),\n",
    "            (\"cat\", categorical_encoder, categorical_cols),\n",
    "            (\"num\", numeric_encoder, numeric_cols),\n",
    "        ],\n",
    "        remainder=\"passthrough\"  # This will pass through other columns not specified\n",
    "    )\n",
    "    return preprocessor, date_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2a9db-0917-4471-ad31-75605ab97db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RMSE_local_pipe(pipe, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    n_folds = 5\n",
    "\n",
    "    # Perform cross-validation and compute the scores\n",
    "    cv_scores_train = cross_val_score(pipe, X_train, y_train, cv=n_folds, scoring='neg_mean_squared_error')\n",
    "    cv_scores_test = cross_val_score(pipe, X_test, y_test, cv=n_folds, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # Convert the scores to root mean squared error\n",
    "    rmse_scores_train = np.sqrt(-cv_scores_train)\n",
    "    rmse_scores_test = np.sqrt(-cv_scores_test)\n",
    "    \n",
    "    print(\n",
    "        f\"Train set, RMSE={np.mean(rmse_scores_train):.2f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Test set, RMSE={np.mean(rmse_scores_test):.2f}\"\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "acfb9648-35a4-4017-a69f-3ca795c9e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get preprocessor\n",
    "preprocessor, date_encoder = preprocessing(X_train_plus_FI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "286655a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set, RMSE=1.74\n",
      "Test set, RMSE=1.45\n"
     ]
    }
   ],
   "source": [
    "# Ridge pipe\n",
    "regressor = Ridge()\n",
    "\n",
    "pipe_Ridge = make_pipeline(date_encoder, preprocessor, regressor)\n",
    "\n",
    "# Predict data and get RMSE\n",
    "get_RMSE_local(pipe_Ridge, X_train_plus_FI, y_train, X_test_plus_FI, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "983a8190-acd4-49f3-8b0c-05216cf90058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set, RMSE=1.70\n",
      "Test set, RMSE=1.42\n"
     ]
    }
   ],
   "source": [
    "# Lasso pipe\n",
    "regressor = Lasso()\n",
    "\n",
    "pipe_Lasso = make_pipeline(date_encoder, preprocessor, regressor)\n",
    "\n",
    "# Predict data and get RMSE\n",
    "get_RMSE_local(pipe_Lasso, X_train_plus_FI, y_train, X_test_plus_FI, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f12a421-a3fe-4c11-a050-40a3757d9cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set, RMSE=1.70\n",
      "Test set, RMSE=1.42\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet pipe\n",
    "regressor = ElasticNet()\n",
    "\n",
    "pipe_ElasticNet = make_pipeline(date_encoder, preprocessor, regressor)\n",
    "\n",
    "# Predict data and get RMSE\n",
    "get_RMSE_local(pipe_ElasticNet, X_train_plus_FI, y_train, X_test_plus_FI, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e7375-d02d-427a-9f9e-daa526501d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestRegressor pipe\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "pipe_RandomForestRegressor = make_pipeline(date_encoder, preprocessor, regressor)\n",
    "\n",
    "# Predict data and get RMSE\n",
    "get_RMSE_local(pipe_RandomForestRegressor, X_train_plus_FI, y_train, X_test_plus_FI, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0dc76e50-8a83-428b-aabf-94fdea990180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.979088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.125360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.108453\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1891\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 3.059432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1941\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.970613\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 827\n",
      "[LightGBM] [Info] Number of data points in the train set: 33286, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.490959\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 33286, number of used features: 162\n",
      "[LightGBM] [Info] Start training from score 3.475406\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 828\n",
      "[LightGBM] [Info] Number of data points in the train set: 33286, number of used features: 162\n",
      "[LightGBM] [Info] Start training from score 3.427394\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 833\n",
      "[LightGBM] [Info] Number of data points in the train set: 33287, number of used features: 162\n",
      "[LightGBM] [Info] Start training from score 3.375315\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 813\n",
      "[LightGBM] [Info] Number of data points in the train set: 33287, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.355753\n",
      "Train set, RMSE=0.69\n",
      "Test set, RMSE=0.55\n"
     ]
    }
   ],
   "source": [
    "# LGMBRegressor pipe\n",
    "regressor = lgb.LGBMRegressor()\n",
    "\n",
    "pipe_LGMBRegressor = make_pipeline(date_encoder, preprocessor, regressor)\n",
    "\n",
    "# Predict data and get RMSE\n",
    "get_RMSE_local(pipe_LGMBRegressor, X_train_plus_FI, y_train, X_test_plus_FI, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1f1a8c36-7da7-491a-ad03-9257875daa80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set, RMSE=0.66\n",
      "Test set, RMSE=0.51\n"
     ]
    }
   ],
   "source": [
    "# XGBRegressor pipe\n",
    "\n",
    "best_params = {'colsample_bytree': 0.6154469128110744,\n",
    "              'gamma': 1,\n",
    "              'learning_rate': 0.09803049874792026,\n",
    "              'max_depth': 9,\n",
    "              'n_estimators': 363,\n",
    "              'subsample': 0.5171942605576092}\n",
    "\n",
    "regressor = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                    n_estimators=best_params['n_estimators'],\n",
    "                    max_depth=best_params['max_depth'],\n",
    "                    learning_rate=best_params['learning_rate'],\n",
    "                    subsample=best_params['subsample'],\n",
    "                    colsample_bytree=best_params['colsample_bytree'],\n",
    "                    gamma=best_params['gamma'],\n",
    "                    seed=42\n",
    "            )\n",
    "\n",
    "pipe_XGBRegressor = make_pipeline(date_encoder, preprocessor, regressor)\n",
    "\n",
    "# Predict data and get RMSE\n",
    "get_RMSE_local(pipe_XGBRegressor, X_train_plus_FI, y_train, X_test_plus_FI, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040716c0-e508-4992-8879-b55f7d3fdb04",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175bcbc-475a-448d-9638-02a02b6432d8",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7529a069-9f82-4463-a5c3-345dc3ee5492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.979088\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.125360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.108453\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1891\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 3.059432\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1941\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.970613\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.979088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.125360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.108453\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010466 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1891\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 3.059432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1941\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.970613\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.979088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.125360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.108453\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1891\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 3.059432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1941\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.970613\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.979088\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.125360\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.108453\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1891\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 3.059432\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1941\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.970613\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.979088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.125360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.108453\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1891\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 3.059432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1941\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.970613\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.979088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.125360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.108453\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1891\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 3.059432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1941\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.970613\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.979088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.125360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.108453\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1891\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 3.059432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1941\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.970613\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.979088\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012984 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.125360\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.108453\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1891\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 3.059432\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1941\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.970613\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.979088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.125360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.108453\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1891\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 3.059432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1941\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.970613\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.979088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.125360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 364130, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 3.108453\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1891\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 3.059432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1941\n",
      "[LightGBM] [Info] Number of data points in the train set: 364131, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 2.970613\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1989\n",
      "[LightGBM] [Info] Number of data points in the train set: 455163, number of used features: 182\n",
      "[LightGBM] [Info] Start training from score 3.048589\n",
      "Best parameters found for LGBM:  {'lgbmregressor__colsample_bytree': 0.5232252063599989, 'lgbmregressor__learning_rate': 0.1315089703802877, 'lgbmregressor__max_depth': 7, 'lgbmregressor__n_estimators': 428, 'lgbmregressor__num_leaves': 26, 'lgbmregressor__subsample': 0.5066324805799333}\n",
      "Lowest RMSE found for LGBM:  0.6860618160438993\n",
      "Test set RMSE of best LGBM model:  0.49583258653055434\n"
     ]
    }
   ],
   "source": [
    "best_params_LGBM = {'lgbmregressor__colsample_bytree': 0.5232252063599989,\n",
    "                    'lgbmregressor__learning_rate': 0.1315089703802877,\n",
    "                    'lgbmregressor__max_depth': 7,\n",
    "                    'lgbmregressor__n_estimators': 428,\n",
    "                    'lgbmregressor__num_leaves': 26,\n",
    "                    'lgbmregressor__subsample': 0.5066324805799333\n",
    "                   }\n",
    "\n",
    "# Define the hyperparameter space for LGBMRegressor\n",
    "param_dist = {\n",
    "    'lgbmregressor__n_estimators': randint(100, 500),\n",
    "    'lgbmregressor__max_depth': randint(3, 10),\n",
    "    'lgbmregressor__learning_rate': uniform(0.01, 0.2),\n",
    "    'lgbmregressor__subsample': uniform(0.5, 0.5),\n",
    "    'lgbmregressor__colsample_bytree': uniform(0.5, 0.5),\n",
    "    'lgbmregressor__num_leaves': randint(20, 40),\n",
    "}\n",
    "\n",
    "# Get preprocessor\n",
    "preprocessor, date_encoder = preprocessing(X_train_plus_FI)\n",
    "pipe_LGMBRegressor = make_pipeline(date_encoder, preprocessor, regressor)\n",
    "\n",
    "# Create a RandomizedSearchCV object for LightGBM\n",
    "random_search_lgbm = RandomizedSearchCV(\n",
    "    estimator= pipe_LGMBRegressor,  # Ensure your pipeline ends with a LGBMRegressor\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Number of parameter settings that are sampled\n",
    "    scoring='neg_root_mean_squared_error',  # Scoring metric to optimize\n",
    "    cv=5,  # Number of folds in cross-validation\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit to the data\n",
    "random_search_lgbm.fit(X_train_plus_FI, y_train)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found for LGBM: \", random_search_lgbm.best_params_)\n",
    "print(\"Lowest RMSE found for LGBM: \", np.abs(random_search_lgbm.best_score_))\n",
    "\n",
    "# To predict and get RMSE on the test set using the best LightGBM model\n",
    "best_model_lgbm = random_search_lgbm.best_estimator_\n",
    "y_pred_lgbm = best_model_lgbm.predict(X_test_plus_FI)\n",
    "rmse_test_lgbm = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))\n",
    "print(\"Test set RMSE of best LGBM model: \", rmse_test_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7778a4-2277-4e34-84e9-37eb94503b72",
   "metadata": {},
   "source": [
    "## XGBregressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f0e7e00-ed7c-44ae-af5d-852ef8397ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'xgbregressor__colsample_bytree': 0.6154469128110744, 'xgbregressor__gamma': 1, 'xgbregressor__learning_rate': 0.09803049874792026, 'xgbregressor__max_depth': 9, 'xgbregressor__n_estimators': 363, 'xgbregressor__subsample': 0.5171942605576092}\n",
      "Lowest RMSE found:  0.677347199973148\n",
      "Test set RMSE of best model:  0.4584068511534211\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter space\n",
    "param_dist = {\n",
    "    'xgbregressor__n_estimators': randint(100, 500),\n",
    "    'xgbregressor__max_depth': randint(3, 10),\n",
    "    'xgbregressor__learning_rate': uniform(0.01, 0.2),\n",
    "    'xgbregressor__subsample': uniform(0.5, 0.5),\n",
    "    'xgbregressor__colsample_bytree': uniform(0.5, 0.5),\n",
    "    'xgbregressor__gamma': [0, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Get preprocessor\n",
    "preprocessor, date_encoder = preprocessing(X_train_plus_FI)\n",
    "pipe_XGBregressor = make_pipeline(date_encoder, preprocessor, regressor)\n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipe_XGBregressor,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Number of parameter settings that are sampled\n",
    "    scoring='neg_root_mean_squared_error',  # Scoring metric to optimize\n",
    "    cv=5,  # Number of folds in cross-validation\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit to the data\n",
    "random_search.fit(X_train_plus_FI, y_train)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.abs(random_search.best_score_))\n",
    "\n",
    "# To predict and get RMSE on the test set using the best model\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_plus_FI)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Test set RMSE of best model: \", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c591331-3880-41b4-acab-c2d1e5723d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
